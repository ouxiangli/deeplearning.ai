{"cells":[{"metadata":{"id":"5FBA7A3834F749B1ACDB1E5647A3EA73","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"这个项目需要配置环境 pydub==0.23.1"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"F6D7852EB05E45828E8446457168C674","mdEditEnable":false,"jupyter":{}},"source":"# 关键字语音识别\n\n欢迎来到本专业的最后一个编程任务！\n\n在本周的视频中，你学习了如何将深度学习应用于语音识别。在此作业中，你将构建语音数据集并实现用于关键词检测（有时也称为唤醒词或触发词检测）的算法。关键词识别是一项技术，可让诸如Amazon Alexa，Google Home，Apple Siri和Baidu DuerOS之类的设备在听到某个特定单词时回应。\n\n对于本练习，我们的触发词将是\"Activate.\"。每次听到你说“激活”时，它都会发出“蜂鸣声”。作业完成后，你将可以录制自己的讲话片段，并在算法检测到你说\"Activate\"时触发提示音。\n\n完成此任务后，也许你还可以将其扩展为在笔记本电脑上运行，以便每次你说“Activate”时，它就会启动你喜欢的应用程序，或者打开房屋中的网络连接灯，或触发其他事件？\n\n![Image Name](https://cdn.kesci.com/upload/image/q7sobmqvu7.png?imageView2/0/w/960/h/960)\n\n在本作业中，你将学习：\n- 构建语音识别项目\n- 合成和处理音频记录以创建训练/开发数据集\n- 训练关键词检测模型并做出预测\n\n让我们开始吧！运行以下单元格以加载要使用的软件包。\n"},{"metadata":{"id":"0AADAC18ADB14823821C05FE720AA533","tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"jupyter":{}},"cell_type":"code","outputs":[],"source":"cd /home/kesci/input/deeplearning172923","execution_count":1},{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"9D9EF3219044439F8BDAB8DA7E21B7EB","scrolled":false,"jupyter":{}},"outputs":[],"source":"import numpy as np\nfrom pydub import AudioSegment\nimport random\nimport sys\nimport io\nimport os\nimport glob\nimport IPython\nfrom td_utils import *\n%matplotlib inline"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"88942DF44988499D85871FB94F56DCBA","mdEditEnable":false,"jupyter":{}},"source":"# 1 合成数据：创建语音数据集\n\n让我们从为触发词检测算法构建数据集开始。语音数据集在理想情况下应尽可能接近要在其上运行它的应用程序。在这种情况下，你想在工作环境（图书馆，家庭，办公室，开放空间...）中检测到\"activate\"一词。因此，你需要在不同的背景声音上混合使用positive词(\"activate\")和negative词（除activate以外的随机词）来创建录音。让我们看看如何创建这样的数据集。\n\n## 1.1 试听数据\n\n你的一位朋友正在帮助你完成这个项目，他们去了该地区各地的图书馆，咖啡馆，餐馆，家庭和办公室，以记录背景噪音以及人们说positive/negative词的音频片段。该数据集包括以各种口音讲话的人。\n\n在raw_data目录中，你可以找到原始音频文件的子集，包括正词，负词和背景噪音。你将使用这些音频文件来合成数据集以训练模型。\"activate\"目录包含人们说\"activate\"一词的正面示例。\"negatives\"目录包含人们说\"activate\"以外的随机单词的否定示例。每个音频记录只有一个字。\"backgrounds\"目录包含10秒的不同环境下的背景噪音片段。\n\n运行下面的单元格以试听一些示例。"},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"58AE549764764E308718172373C73317","scrolled":false,"jupyter":{}},"outputs":[],"source":"IPython.display.Audio(\"./raw_data/activates/1.wav\")"},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"35C5688CA5F34EF08B05829EF8D2CF83","scrolled":false,"jupyter":{}},"outputs":[],"source":"IPython.display.Audio(\"./raw_data/negatives/4.wav\")"},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"8D930AFF1DDD40F9BBA278F008B1AB7D","jupyter":{}},"outputs":[],"source":"IPython.display.Audio(\"./raw_data/backgrounds/1.wav\")"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"1953CCA4807E45218187121AE34E3473","mdEditEnable":false,"jupyter":{}},"source":"你将使用这三种类型的记录(positives/negatives/backgrounds)来创建标记的数据集。"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"17CC41002549453A9A93F41059259796","jupyter":{},"mdEditEnable":false},"source":"## 1.2 从录音到频谱图\n\n录音到底是什么？麦克风记录随时间变化很小的气压，而这些气压的微小变化也会使你的耳朵感觉到声音。你可以认为录音是一长串数字，用于测量麦克风检测到的气压变化很小。我们将使用以44100Hz（或44100赫兹）采样的音频。这意味着麦克风每秒可以为我们提供44100个号码。因此，一个10秒的音频剪辑由441000个数字表示(= $10 \\times 44100$)。\n\n从音频的这种“原始”表示中很难弄清是否说了\"activate\"这个词。为了帮助你的序列模型更轻松地学习检测触发词，我们将计算音频的*spectrogram*。频谱图告诉我们音频片段在某个时刻存在多少不同的频率。\n\n（如果你曾经在信号处理或傅立叶变换方面上过高级课程，则可以通过在原始音频信号上滑动一个窗口来计算频谱图，并使用傅立叶变换来计算每个窗口中最活跃的频率。如果你不理解前面的句子，也不用担心。）\n\n让我们来看一个例子。"},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"0AC7224CC6D946ED82C020CC7B9285C6","scrolled":false,"jupyter":{}},"outputs":[],"source":"IPython.display.Audio(\"audio_examples/example_train.wav\")"},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E06F4D38F10646548917544C4DFBF7EB","scrolled":false,"jupyter":{}},"outputs":[],"source":"x = graph_spectrogram(\"audio_examples/example_train.wav\")"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"C61C5C9C4EE6481E8E59B46A2CED890B","mdEditEnable":false,"jupyter":{}},"source":"上图表示在多个时间步长（x轴）上每个频率（y轴）的活跃程度。\n\n![Image Name](https://cdn.kesci.com/upload/image/q22m7i3xmz.png?imageView2/0/w/960/h/960)\n\n**图1**：录音的频谱图，其中的颜色表示在不同的时间点音频中不同频率出现（响亮）的程度。绿色方块表示某个频率在音频剪辑（扬声器）中更活跃或更活跃。蓝色方块表示较不活跃的频率。\n\n输出频谱图的尺寸取决于频谱图软件的超参数和输入的长度。在此笔记本中，我们将使用10秒的音频剪辑作为训练示例的“标准长度”。频谱图的时间步数为5511。稍后你将看到频谱图将是网络中的输入$x$，因此$T_x = 5511$。"},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"DBFE080ED0A0460E8C500D9C4D53B149","scrolled":false,"jupyter":{}},"outputs":[],"source":"_, data = wavfile.read(\"audio_examples/example_train.wav\")\nprint(\"Time steps in audio recording before spectrogram\", data[:,0].shape)\nprint(\"Time steps in input after spectrogram\", x.shape)"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"C46EC7D506AE423980C316C3A5249E5A","mdEditEnable":false,"jupyter":{}},"source":"现在，你可以定义："},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E59BDF8100F845BEB01E2F1975BE5A2B","scrolled":false,"jupyter":{}},"outputs":[],"source":"Tx = 5511 # The number of time steps input to the model from the spectrogram\nn_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"9B8B261D294F435A896829E28729B05A","mdEditEnable":false,"jupyter":{}},"source":"请注意，即使将10秒作为我们的默认训练示例长度，也可以将10秒的时间离散化为不同数量的值。你已经看到441000（原始音频）和5511（频谱图）。在前一种情况下，每个步骤代表 $10/441000 \\approx 0.000023$秒。在第二种情况下，每个步骤代表$10/5511 \\approx 0.0018$秒。\n\n对于10秒的音频，你将在此作业中看到的关键值为：\n\n- $441000$（原始音频）\n- $5511 = T_x$（频谱图输出，以及神经网络的输入维数）。\n- $10000$（用`pydub`模块来合成音频）\n- $1375 = T_y$（要构建的GRU输出中的步骤数）。\n\n请注意，这些表示中的每个表示都恰好对应10秒的时间。只是他们在不同程度上离散化了他们。所有这些都是超参数，可以更改（441000除外，这是麦克风函数）。我们选择的值在语音系统使用的标准范围内。\n\n上面的$T_y = 1375$数字意味着对于模型的输出，我们将10s离散为1375个时间间隔（每个时间间隔的长度为$10/1375 \\approx 0.0072$s），并尝试针对每个时间间隔预测是否有人最近说完“activate”。\n\n上面的10000对应于将10秒剪辑离散化为10/10000 = 0.001秒迭代。0.001秒也称为1毫秒或1ms。因此，当我们说要按照1ms的间隔离散时，这意味着我们正在使用10,000个步长。\n"},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"1F8088A91F194515AA0659F7E10A37BA","scrolled":false,"jupyter":{}},"outputs":[],"source":"Ty = 1375 # The number of time steps in the output of our model"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"4A55F85075344A1B97A85F804C41BAF9","mdEditEnable":false,"jupyter":{}},"source":"## 1.3 生成单个训练示例\n\n由于语音数据很难获取和标记，因此你将使用激活，否定和背景的音频片段来合成训练数据。录制很多带有随机\"activates\"内容的10秒音频剪辑非常慢。取而代之的是，录制许多肯定词和否定词以及分别记录背景噪音（或从免费的在线资源下载背景噪音）会变得更加容易。\n\n要合成一个训练示例，你将：\n\n- 随机选择10秒钟的背景音频剪辑\n- 将\"activates\"的0-4个音频片段随机插入此10秒的片段中\n- 将10个否定词的音频剪辑随机插入此10秒剪辑中\n\n因为你已经将\"activates\"一词合成到了背景剪辑中，所以你确切知道\"activates\"在10秒剪辑中何时出现。稍后你将看到，这也使得生成标签 $y^{\\langle t \\rangle}$ 更加容易。\n\n你将使用pydub包来处理音频。Pydub将原始音频文件转换为Pydub数据结构的列表（在此处了解详细信息并不重要）。Pydub使用1毫秒作为离散时间间隔（1毫秒等于1毫秒= 1/1000秒），这也是为什么始终以10,000步表示10秒剪辑的原因。"},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"4AFE46C6DC6645A6898DD964A08E4B28","scrolled":false,"jupyter":{}},"outputs":[],"source":"# Load audio segments using pydub \nactivates, negatives, backgrounds = load_raw_audio()\n\nprint(\"background len: \" + str(len(backgrounds[0])))    # Should be 10,000, since it is a 10 sec clip\nprint(\"activate[0] len: \" + str(len(activates[0])))     # Maybe around 1000, since an \"activate\" audio clip is usually around 1 sec (but varies a lot)\nprint(\"activate[1] len: \" + str(len(activates[1])))     # Different \"activate\" clips can have different lengths "},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"3CD3AD62F2444F2B89D8AC1A48599215","mdEditEnable":false,"jupyter":{}},"source":"**在背景上叠加正/负词**：\n\n给定一个10秒的背景剪辑和一个简短的音频剪辑(positive or negative word)，你需要能够将单词的简短音频剪辑“添加”或“插入”到背景上。为确保插入背景的音频片段不重叠，你将跟踪以前插入的音频片段的时间。你将在背景中插入多个正/负词剪辑，而又不想在与先前添加的另一个剪辑重叠的位置插入\"activate\"或随机词。\n\n为了清楚起见，当你在10秒的咖啡馆噪音片段中插入1秒的 \"activate\" 时，你最终会得到一个10秒的片段，听起来像有人在咖啡馆中说 \"activate\"，背景咖啡馆噪音中叠加了 \"activate\" 。注意你*没有*以11秒的剪辑结尾。稍后你将看到pydub如何帮助你执行此操作。\n\n**在叠加的同时创建标签**：\n\n还记得标签$y^{\\langle t \\rangle}$代表某人是否刚刚说完\"activate.\"。给定一个背景剪辑，我们可以为所有$t$初始化$y^{\\langle t \\rangle}=0$，因为该剪辑不包含任何\"activates.\"。\n\n当插入或覆盖\"activate\"剪辑时，还将更新$y^{\\langle t \\rangle}$的标签，以便输出的50个步骤现在具有目标标签1。你将训练GRU来检测何时某人*完成*说\"activate\"。例如，假设合成的\"activate\"剪辑在10秒音频中的5秒标记处结束-恰好在剪辑的一半处。回想一下 $T_y = 1375$，因此时间步长$ 687 = `int（1375 * 0.5）`$对应于进入音频5秒的时刻。因此，你将设置$y^{\\langle 688 \\rangle} = 1$。此外，如果GRU在此刻之后的短时间内（在内部）在任何地方检测到\"activate\"，你将非常满意，因此我们实际上将标签$y^{\\langle t \\rangle}$的50个连续值设置为1。我们有$y^{\\langle 688 \\rangle} = y^{\\langle 689 \\rangle} = \\cdots = y^{\\langle 737 \\rangle} = 1$。\n\n\n这是合成训练数据的另一个原因：如上所述，生成这些标签$y^{\\langle t \\rangle}$相对简单。相反，如果你在麦克风上录制了10秒的音频，那么一个人收听它并在 \"activate\" 完成时准确手动进行标记会非常耗时。\n\n下图显示了标签$y^{\\langle t \\rangle}$，对于我们插入了\"activate\", \"innocent\", activate\", \"baby\"的剪辑，请注意，正标签“1”是关联的只用positive的词。\n\n![Image Name](https://cdn.kesci.com/upload/image/q22m8zybh8.png?imageView2/0/w/960/h/960)\n\n**图2 **\n\n要实现合成训练集过程，你将使用以下帮助函数。所有这些函数将使用1ms的离散时间间隔，因此将10秒的音频离散化为10,000步。\n\n1. `get_random_time_segment（segment_ms）`在我们的背景音频中获得随机的时间段\n1. `is_overlapping（segment_time，existing_segments）`检查时间段是否与现有时间段重叠\n1. `insert_audio_clip（background，audio_clip，existing_times）`使用`get_random_time_segment`和`is_overlapping`在我们的背景音频中随机插入一个音频片段。\n1. `insert_ones（y，segment_end_ms）`在我们的标签向量y的\"activate\"词之后插入1。\n"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"2C8AE07D70BE4E72BE54E15220CB05F6","mdEditEnable":false,"jupyter":{}},"source":"函数 `get_random_time_segment(segment_ms)`返回一个随机的时间段，我们可以在其中插入持续时间为`segment_ms`的音频片段。 通读代码以确保你了解它在做什么。"},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"85F8013983C34ADB8DC2083C8B034A18","scrolled":false,"jupyter":{}},"outputs":[],"source":"def get_random_time_segment(segment_ms):\n    \"\"\"\n    Gets a random time segment of duration segment_ms in a 10,000 ms audio clip.\n    \n    Arguments:\n    segment_ms -- the duration of the audio clip in ms (\"ms\" stands for \"milliseconds\")\n    \n    Returns:\n    segment_time -- a tuple of (segment_start, segment_end) in ms\n    \"\"\"\n    \n    segment_start = np.random.randint(low=0, high=10000-segment_ms)   # Make sure segment doesn't run past the 10sec background \n    segment_end = segment_start + segment_ms - 1\n    \n    return (segment_start, segment_end)"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"C9112F5785C149BDB26281C0D06D2E0C","mdEditEnable":false,"jupyter":{}},"source":"接下来，假设你在（1000,1800）和（3400,4500）段插入了音频剪辑。即第一个片段开始于1000步，结束于1800步。现在，如果我们考虑在（3000,3600）插入新的音频剪辑，这是否与先前插入的片段之一重叠？在这种情况下，（3000,3600）和（3400,4500）重叠，因此我们应该决定不要在此处插入片段。\n\n出于此函数的目的，将（100,200）和（200,250）定义为重叠，因为它们在时间步200处重叠。但是，（100,199）和（200,250）是不重叠的。\n\n**练习**：实现`is_overlapping（segment_time，existing_segments）`来检查新的时间段是否与之前的任何时间段重叠。你需要执行2个步骤：\n\n1. 创建一个“False”标志，如果发现有重叠，以后将其设置为“True”。\n1. 循环遍历previous_segments的开始和结束时间。将这些时间与细分的开始时间和结束时间进行比较。如果存在重叠，请将（1）中定义的标志设置为True。你可以使用：\n```python\nfor ....:\n        if ... <= ... and ... >= ...:\n            ...\n```\n提示：如果该段在上一个段结束之前开始，并且该段在上一个段开始之后结束，则存在重叠。"},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E8EC0E66703F44498B0812B8B3E80C21","scrolled":false,"jupyter":{}},"outputs":[],"source":"# GRADED FUNCTION: is_overlapping\r\n\r\ndef is_overlapping(segment_time, previous_segments):\r\n    \"\"\"\r\n    Checks if the time of a segment overlaps with the times of existing segments.\r\n    \r\n    Arguments:\r\n    segment_time -- a tuple of (segment_start, segment_end) for the new segment\r\n    previous_segments -- a list of tuples of (segment_start, segment_end) for the existing segments\r\n    \r\n    Returns:\r\n    True if the time segment overlaps with any of the existing segments, False otherwise\r\n    \"\"\"\r\n    \r\n    segment_start, segment_end = segment_time\r\n    \r\n    ### START CODE HERE ### (≈ 4 line)\r\n    # Step 1: Initialize overlap as a \"False\" flag. (≈ 1 line)\r\n    overlap = False\r\n    \r\n    # Step 2: loop over the previous_segments start and end times.\r\n    # Compare start/end times and set the flag to True if there is an overlap (≈ 3 lines)\r\n    for previous_start, previous_end in previous_segments:\r\n        if segment_start >= previous_start and segment_start <= previous_end or segment_end >= previous_start and segment_end <= previous_end :\r\n            overlap = True\r\n    ### END CODE HERE ###\r\n\r\n    return overlap"},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"DF4E65343CB043C29F76B50195781189","jupyter":{}},"outputs":[],"source":"overlap1 = is_overlapping((950, 1430), [(2000, 2550), (260, 949)])\noverlap2 = is_overlapping((2305, 2950), [(824, 1532), (1900, 2305), (3424, 3656)])\nprint(\"Overlap 1 = \", overlap1)\nprint(\"Overlap 2 = \", overlap2)"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"1BD3C841D77B4C49883258B9A3EBBBB6","mdEditEnable":false,"jupyter":{}},"source":"**预期输出**:\nOverlap 1 =  False\nOverlap 2 =  True"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"8ADCA2270C3847B78C19A244F743D68F","mdEditEnable":false,"jupyter":{}},"source":"现在，让我们使用以前的辅助函数在10秒钟的随机时间将新的音频片段插入到背景中，但是要确保任何新插入的片段都不会与之前的片段重叠。\n\n**练习**：实现`insert_audio_clip()`以将音频片段叠加到背景10秒片段上。你将需要执行4个步骤：\n\n1. 以ms为单位获取正确持续时间的随机时间段。\n1. 确保该时间段与之前的任何时间段均不重叠。如果重叠，则返回步骤1并选择一个新的时间段。\n1. 将新时间段添加到现有时间段列表中，以便跟踪你插入的所有时间段。\n1. 使用pydub在背景上覆盖音频片段。我们已经为你实现了这一点。"},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E3C3769A871F464A819FD2EA67A0989E","scrolled":false,"jupyter":{}},"outputs":[],"source":"# GRADED FUNCTION: insert_audio_clip\r\n\r\ndef insert_audio_clip(background, audio_clip, previous_segments):\r\n    \"\"\"\r\n    Insert a new audio segment over the background noise at a random time step, ensuring that the \r\n    audio segment does not overlap with existing segments.\r\n    \r\n    Arguments:\r\n    background -- a 10 second background audio recording.  \r\n    audio_clip -- the audio clip to be inserted/overlaid. \r\n    previous_segments -- times where audio segments have already been placed\r\n    \r\n    Returns:\r\n    new_background -- the updated background audio\r\n    \"\"\"\r\n    \r\n    # Get the duration of the audio clip in ms\r\n    segment_ms = len(audio_clip)\r\n    \r\n    ### START CODE HERE ### \r\n    # Step 1: Use one of the helper functions to pick a random time segment onto which to insert \r\n    # the new audio clip. (≈ 1 line)\r\n    segment_time = get_random_time_segment(segment_ms)\r\n    # Step 2: Check if the new segment_time overlaps with one of the previous_segments. If so, keep \r\n    # picking new segment_time at random until it doesn't overlap. (≈ 2 lines)\r\n    while is_overlapping(segment_time,previous_segments):\r\n        segment_time = get_random_time_segment(segment_ms)\r\n\r\n    # Step 3: Add the new segment_time to the list of previous_segments (≈ 1 line)\r\n    previous_segments.append(segment_time)\r\n    ### END CODE HERE ###\r\n    \r\n    # Step 4: Superpose audio segment and background\r\n    new_background = background.overlay(audio_clip, position = segment_time[0])\r\n    \r\n    return new_background, segment_time"},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"DE8E980917094013AC217E39830B6744","scrolled":false,"jupyter":{}},"outputs":[],"source":"np.random.seed(5)\naudio_clip, segment_time = insert_audio_clip(backgrounds[0], activates[0], [(3790, 4400)])\naudio_clip.export(\"insert_test.wav\", format=\"wav\")\nprint(\"Segment Time: \", segment_time)\nIPython.display.Audio(\"insert_test.wav\")"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"03D4C3193DE747B48DD54679D85B9BE1","mdEditEnable":false,"jupyter":{}},"source":"**预期输出**\nSegment Time:  (2915, 3569)"},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"B0D424AD45AA4E919E786346FA70448D","scrolled":false,"jupyter":{}},"outputs":[],"source":"# Expected audio\nIPython.display.Audio(\"audio_examples/insert_reference.wav\")"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"79DFDC7E22AC4788AA1319DEB749FC5A","mdEditEnable":false,"jupyter":{}},"source":"最后，假设你刚刚插入了\"activate.\" ，则执行代码以更新标签$y^{\\langle t \\rangle}$。在下面的代码中，由于$T_y = 1375$，所以`y`是一个 `(1,1375)`维向量。\n\n如果\"activate\"在时间步骤$t$结束，则设置$y^{\\langle t+1 \\rangle} = 1$以及最多49个其他连续值。但是，请确保你没有用完数组的末尾并尝试更新 `y[0][1375]`，由于$T_y = 1375$，所以有效索引是 `y[0][0]` 至`y[0][1374]`。因此，如果\"activate\" 在1370步结束，则只会得到`y[0][1371] = y[0][1372] = y[0][1373] = y[0][1374] = 1`\n\n**练习**：实现`insert_ones()`。你可以使用for循环。（如果你是python的slice运算的专家，请随时使用切片对此向量化。）如果段以`segment_end_ms`结尾（使用10000步离散化），请将其转换为输出$y$的索引（使用$1375$步离散化），我们将使用以下公式：\n```\n    segment_end_y = int(segment_end_ms * Ty / 10000.0)\n```"},{"cell_type":"code","execution_count":18,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"324C92C7F80B4DD685B9F45BE28AA8C5","scrolled":false,"jupyter":{}},"outputs":[],"source":"# GRADED FUNCTION: insert_ones\r\n\r\ndef insert_ones(y, segment_end_ms):\r\n    \"\"\"\r\n    Update the label vector y. The labels of the 50 output steps strictly after the end of the segment \r\n    should be set to 1. By strictly we mean that the label of segment_end_y should be 0 while, the\r\n    50 followinf labels should be ones.\r\n    \r\n    \r\n    Arguments:\r\n    y -- numpy array of shape (1, Ty), the labels of the training example\r\n    segment_end_ms -- the end time of the segment in ms\r\n    \r\n    Returns:\r\n    y -- updated labels\r\n    \"\"\"\r\n    \r\n    # duration of the background (in terms of spectrogram time-steps)\r\n    segment_end_y = int(segment_end_ms * Ty / 10000.0)\r\n    \r\n    # Add 1 to the correct index in the background label (y)\r\n    ### START CODE HERE ### (≈ 3 lines)\r\n    for i in range(segment_end_y+1, segment_end_y+51):\r\n        if i < Ty:\r\n            y[0, i] = 1\r\n    ### END CODE HERE ###\r\n    \r\n    return y"},{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"F27906A73C5C4136A1286016024A3A8F","scrolled":false,"jupyter":{}},"outputs":[],"source":"arr1 = insert_ones(np.zeros((1, Ty)), 9700)\nplt.plot(insert_ones(arr1, 4251)[0,:])\nprint(\"sanity checks:\", arr1[0][1333], arr1[0][634], arr1[0][635])"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"538A7D42B21E4F46BCECB95B1FEB7FD3","mdEditEnable":false,"jupyter":{}},"source":"**预期输出**\nsanity checks: 0.0 1.0 0.0\n\n![Image Name](https://cdn.kesci.com/upload/image/q22mc3ekca.png?imageView2/0/w/960/h/960)\n"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"C70A16B5959E4EA8BE8EB9384CC0A595","mdEditEnable":false,"jupyter":{}},"source":"最后，你可以使用`insert_audio_clip`和 `insert_ones`来创建一个新的训练示例。\n\n**练习**：实现`create_training_example()`。你需要执行以下步骤：\n\n1. 将标签向量$y$初始化为维度为$(1, T_y)$的零numpy数组\n1. 将现有段的集合初始化为一个空列表\n1. 随机选择0到4个\"activate\"音频剪辑，并将其插入10秒剪辑中。还要在标签向量$y$的正确位置插入标签。\n1. 随机选择0到2个负音频片段，并将其插入10秒片段中。\n"},{"cell_type":"code","execution_count":35,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"C05667A0DF5F436E9758D2005FA442B3","scrolled":false,"jupyter":{}},"outputs":[],"source":"# GRADED FUNCTION: create_training_example\n\ndef create_training_example(background, activates, negatives):\n    \"\"\"\n    Creates a training example with a given background, activates, and negatives.\n    \n    Arguments:\n    background -- a 10 second background audio recording\n    activates -- a list of audio segments of the word \"activate\"\n    negatives -- a list of audio segments of random words that are not \"activate\"\n    \n    Returns:\n    x -- the spectrogram of the training example\n    y -- the label at each time step of the spectrogram\n    \"\"\"\n    \n    # Set the random seed\n    np.random.seed(18)\n    \n    # Make background quieter\n    background = background - 20\n\n    ### START CODE HERE ###\n    # Step 1: Initialize y (label vector) of zeros (≈ 1 line)\n    y = np.zeros((1, Ty))\n\n    # Step 2: Initialize segment times as empty list (≈ 1 line)\n    previous_segments = []\n    ### END CODE HERE ###\n    \n    # Select 0-4 random \"activate\" audio clips from the entire list of \"activates\" recordings\n    number_of_activates = np.random.randint(0, 5)\n    random_indices = np.random.randint(len(activates), size=number_of_activates)\n    random_activates = [activates[i] for i in random_indices]\n    \n    ### START CODE HERE ### (≈ 3 lines)\n    # Step 3: Loop over randomly selected \"activate\" clips and insert in background\n    for random_activate in random_activates:\n        # Insert the audio clip on the background\n        background, segment_time = insert_audio_clip(background, random_activate, previous_segments)\n        # Retrieve segment_start and segment_end from segment_time\n        segment_start, segment_end = segment_time\n        # Insert labels in \"y\"\n        y = insert_ones(y, segment_end)\n    ### END CODE HERE ###\n\n    # Select 0-2 random negatives audio recordings from the entire list of \"negatives\" recordings\n    number_of_negatives = np.random.randint(0, 3)\n    random_indices = np.random.randint(len(negatives), size=number_of_negatives)\n    random_negatives = [negatives[i] for i in random_indices]\n\n    ### START CODE HERE ### (≈ 2 lines)\n    # Step 4: Loop over randomly selected negative clips and insert in background\n    for random_negative in random_negatives:\n        # Insert the audio clip on the background \n        background, _ = insert_audio_clip(background, random_negative, previous_segments)\n    ### END CODE HERE ###\n    \n    # Standardize the volume of the audio clip \n    background = match_target_amplitude(background, -20.0)\n\n    # Export new training example \n    file_handle = background.export(\"train\" + \".wav\", format=\"wav\")\n    print(\"File (train.wav) was saved in your directory.\")\n    \n    # Get and plot spectrogram of the new recording (background with superposition of positive and negatives)\n    x = graph_spectrogram(\"train.wav\")\n    \n    return x, y"},{"cell_type":"code","execution_count":36,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"4689D2ABC15049258CA6272285375D2C","scrolled":false,"jupyter":{}},"outputs":[],"source":"x, y = create_training_example(backgrounds[0], activates, negatives)"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"08DD6AC7B08D44518E02C8E74D7CFBC1","mdEditEnable":false,"jupyter":{}},"source":"**预期输出**\n\n![Image Name](https://cdn.kesci.com/upload/image/q22mcw1yld.png?imageView2/0/w/960/h/960)"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"CF748A0C020D4A4783E49E31A59DF9B5","mdEditEnable":false,"jupyter":{}},"source":"现在，你可以听听你创建的训练示例，并将其与上面生成的频谱图进行比较。"},{"cell_type":"code","execution_count":22,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"513F7DDD99734D5497423F308799959F","scrolled":false,"jupyter":{}},"outputs":[],"source":"IPython.display.Audio(\"train.wav\")"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"1BEC10D4888D411788DFA85154DC507D","mdEditEnable":false,"jupyter":{}},"source":"**预期输出**"},{"cell_type":"code","execution_count":23,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"3A4C449D71CC480DA4F0304B38AA8816","scrolled":false,"jupyter":{}},"outputs":[],"source":"IPython.display.Audio(\"audio_examples/train_reference.wav\")"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"3D6DEE7BF45F4D76A2234F13AC9A1BA9","mdEditEnable":false,"jupyter":{}},"source":"最后，你可以为生成的训练示例绘制关联的标签。"},{"cell_type":"code","execution_count":24,"metadata":{"collapsed":false,"scrolled":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"34B3BDF2EC76484484F9DC093144B417","jupyter":{}},"outputs":[],"source":"plt.plot(y[0])"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"BF866B4CD9E94F03AC0A650C73ACF919","mdEditEnable":false,"jupyter":{}},"source":"**预期输出**\n\n![Image Name](https://cdn.kesci.com/upload/image/q22ms1qt3j.png?imageView2/0/w/960/h/960)\n"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"777FAA825B6D45A89346A19A91D901DD","mdEditEnable":false,"jupyter":{}},"source":"## 1.4 完整训练集\n\n现在，你已经实现了生成单个训练示例所需的代码。我们使用此过程生成了大量的训练集。为了节省时间，我们已经生成了一组训练示例。"},{"cell_type":"code","execution_count":25,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"3510E01837944A779AF373237F7603B1","scrolled":false,"jupyter":{}},"outputs":[],"source":"# Load preprocessed training examples\nX = np.load(\"./XY_train/X.npy\")\nY = np.load(\"./XY_train/Y.npy\")"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"BCE7A1EAF8D540D89B290D4ED9406218","mdEditEnable":false,"jupyter":{}},"source":"## 1.5 开发集\n\n为了测试我们的模型，我们记录了包含25个示例的开发集。在合成训练数据的同时，我们希望使用与实际输入相同的分布来创建开发集。因此，我们录制了25个10秒钟的人们说\"activate\"和其他随机单词的音频剪辑，并手动标记了它们。这遵循课程3中描述的原则，即我们应该将开发集创建为与测试集尽可能相似。这就是为什么我们的开发人员使用真实音频而非合成音频的原因。\n"},{"cell_type":"code","execution_count":26,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"C8F0020A9D924BCB8B51D35A99E0EE7C","scrolled":false,"jupyter":{}},"outputs":[],"source":"# Load preprocessed dev set examples\nX_dev = np.load(\"./XY_dev/X_dev.npy\")\nY_dev = np.load(\"./XY_dev/Y_dev.npy\")"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"90A18EF7440E481BAC6223747ECEEBFF","mdEditEnable":false,"jupyter":{}},"source":"# 2 模型\n\n现在，你已经建立了数据集，让我们编写和训练关键字识别模型！\n\n该模型将使用一维卷积层，GRU层和密集层。让我们加载在Keras中使用这些层的软件包。加载可能需要一分钟。"},{"cell_type":"code","execution_count":27,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"546536D4630040B09D4CC49BE9F6E174","scrolled":false,"jupyter":{}},"outputs":[],"source":"from keras.callbacks import ModelCheckpoint\nfrom keras.models import Model, load_model, Sequential\nfrom keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D\nfrom keras.layers import GRU, Bidirectional, BatchNormalization, Reshape\nfrom keras.optimizers import Adam"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"B762FCF833ED40098A22A9639279DECF","mdEditEnable":false,"jupyter":{}},"source":"## 2.1 建立模型\n\n这是我们将使用的模型架构。花一些时间查看模型，看看它是否合理。\n\n![Image Name](https://cdn.kesci.com/upload/image/q22msmm376.png?imageView2/0/w/960/h/960)\n\n**图3 **\n\n该模型的一个关键步骤是一维卷积步骤（图3的底部附近）。它输入5511步频谱图，并输出1375步，然后由多层进一步处理以获得最终的 $T_y = 1375$步输出。该层的作用类似于你在课程4中看到的2D卷积，其作用是提取低级特征，然后生成较小尺寸的输出。\n\n通过计算，一维转换层还有助于加快模型的速度，因为现在GRU只需要处理1375个时间步，而不是5511个时间步。这两个GRU层从左到右读取输入序列，然后最终使用dense+sigmoid层对$y^{\\langle t \\rangle}$进行预测。因为$y$是二进制值（0或1），所以我们在最后一层使用Sigmoid输出来估计输出为1的机率，对应用户刚刚说过\"activate\"。\n\n请注意，我们使用的是单向RNN，而不是双向RNN。这对于关键字检测确实非常重要，因为我们希望能够在说出触发字后立即检测到触发字。如果我们使用双向RNN，则必须等待记录整个10秒的音频，然后才能知道在音频剪辑的第一秒中是否说了\"activate\"。"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"D40D6AE698C3439D8E6B02736C4347EC","mdEditEnable":false,"jupyter":{}},"source":"可以通过四个步骤来实现模型：\n    \n**步骤1**：CONV层。使用`Conv1D()`和196个滤波器来实现，\n滤波器大小为15（`kernel_size = 15`），步幅为4。 [[See documentation.](https://keras.io/layers/convolutional/#conv1d)]\n\n**步骤2**：第一个GRU层。要生成GRU层，请使用：\n```\nX = GRU(units = 128, return_sequences = True)(X)\n```\n设置`return_sequences = True`可以确保所有GRU的隐藏状态都被feed到下一层。请记住，在Dropout和BatchNorm层之后进行此操作。\n\n**步骤3**：第二个GRU层。这类似于先前的GRU层（请记住使用`return_sequences = True`），但是有一个额外的dropout层。\n\n**步骤4**：按以下步骤创建一个时间分布的密集层：\n```\nX = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X)\n```\n这将创建一个紧随其后的Sigmoid密集层，因此用于密集层的参数对于每个时间步都是相同的。[[See documentation](https://keras.io/layers/wrappers/).]\n\n**练习**：实现`model()`，其架构如图3所示。"},{"cell_type":"code","execution_count":32,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"A090C11871BE4AEC88B3654B210685E1","scrolled":false,"jupyter":{}},"outputs":[],"source":"# GRADED FUNCTION: model\n\ndef model(input_shape):\n    \"\"\"\n    Function creating the model's graph in Keras.\n    \n    Argument:\n    input_shape -- shape of the model's input data (using Keras conventions)\n\n    Returns:\n    model -- Keras model instance\n    \"\"\"\n    \n    X_input = Input(shape = input_shape)\n    \n    ### START CODE HERE ###\n    \n    # Step 1: CONV layer (≈4 lines)\n    X = Conv1D(196, 15, strides=4)(X_input)                    # CONV1D\n    X = BatchNormalization()(X)                                # Batch normalization\n    X = Activation('relu')(X)                                  # ReLu activation\n    X = Dropout(0.8)(X)                                        # dropout (use 0.8)\n\n    # Step 2: First GRU Layer (≈4 lines)\n    X = GRU(units = 128, return_sequences=True)(X)             # GRU (use 128 units and return the sequences)\n    X = Dropout(0.8)(X)                                        # dropout (use 0.8)\n    X = BatchNormalization()(X)                                # Batch normalization\n    \n    # Step 3: Second GRU Layer (≈4 lines)\n    X = GRU(units = 128, return_sequences=True)(X)             # GRU (use 128 units and return the sequences)\n    X = Dropout(0.8)(X)                                        # dropout (use 0.8)\n    X = BatchNormalization()(X)                                # Batch normalization\n    X = Dropout(0.8)(X)                                        # dropout (use 0.8)\n    \n    # Step 4: Time-distributed dense layer (≈1 line)\n    X = TimeDistributed(Dense(1, activation = \"sigmoid\"))(X) # time distributed  (sigmoid)\n\n    ### END CODE HERE ###\n\n    model = Model(inputs = X_input, outputs = X)\n    \n    return model  "},{"cell_type":"code","execution_count":33,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"CBEABFC794BD4EBD81D8ECF2F6F5268F","scrolled":false,"jupyter":{}},"outputs":[],"source":"model = model(input_shape = (Tx, n_freq))"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"6076E2ACD9074724990669486B85DA41","mdEditEnable":false,"jupyter":{}},"source":"让我们输出模型总结以查看维度。"},{"cell_type":"code","execution_count":34,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"E9B37BEA661A4F1C8CA2327392F89806","scrolled":false,"jupyter":{}},"outputs":[],"source":"model.summary()"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"54F1318EEBDF4AC188DF1C164D351217","mdEditEnable":false,"jupyter":{}},"source":"**预期输出**:\n\n| **Total params**         | 522,561 |\n| ------------------------ | ------- |\n| **Trainable params**     | 521,657 |\n| **Non-trainable params** | 904     |"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"3F0108667F5749D1A209F6FA6682062A","jupyter":{},"mdEditEnable":false},"source":"网络的输出为（None，1375，1），输入为（None，5511，101）。Conv1D将步数从频谱图上的5511减少到1375。"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"930A5EDC27E0404E801482E9BDA69D59","jupyter":{},"mdEditEnable":false},"source":"\n## 2.2 拟合模型"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"84D6DE36C0DD4ADE8D8708443C010700","jupyter":{},"mdEditEnable":false},"source":"关键词检测需要很长时间来训练。为了节省时间，我们已经使用你上面构建的架构在GPU上训练了大约3个小时的模型，并提供了大约4000个示例的大型训练集。让我们加载模型吧。"},{"cell_type":"code","execution_count":37,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"8EFB63A4731146648DA6C902F3AA47B0","scrolled":false,"jupyter":{}},"outputs":[],"source":"model = load_model('./models/tr_model.h5')"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"7A694C8B51F84C9396B67216941480FE","mdEditEnable":false,"jupyter":{}},"source":"你可以使用Adam优化器和二进制交叉熵损失进一步训练模型，如下所示。这将很快运行，因为我们只训练一个epoch，并提供26个例子的小训练集。"},{"cell_type":"code","execution_count":38,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"75AC5DC61C8D4666BDADA02C5CB452AA","scrolled":false,"jupyter":{}},"outputs":[],"source":"opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"accuracy\"])"},{"cell_type":"code","execution_count":39,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"6D7F71B1A0DD4AC18FDB4FCD8F00DE59","scrolled":false,"jupyter":{}},"outputs":[],"source":"model.fit(X, Y, batch_size = 5, epochs=1)"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"3B176BBD522547EC8F294D68751FC61E","mdEditEnable":false,"jupyter":{}},"source":"## 2.3 测试模型\n\n最后，让我们看看你的模型在开发集上的表现。"},{"cell_type":"code","execution_count":40,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"5246BA4C2A794FEDA3BEB4171CA12D84","scrolled":false,"jupyter":{}},"outputs":[],"source":"loss, acc = model.evaluate(X_dev, Y_dev)\nprint(\"Dev set accuracy = \", acc)"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"1D46D0D9BEB746DD981DA36C6E1E7C14","jupyter":{},"mdEditEnable":false},"source":"看起来不错！但是，精度并不是这项任务的重要指标，因为标签严重偏斜到0，因此仅输出0的神经网络的精度将略高于90％。我们可以定义更有用的指标，例如F1得分或“精确度/召回率”。但是，我们不要在这里使用它，而只是凭经验看看模型是如何工作的。"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"A63D18609BAC4E89A798B823DE43E453","jupyter":{},"mdEditEnable":false},"source":"# 3 预测\n\n现在，你已经建立了用于触发词检测的工作模型，让我们使用它来进行预测吧。此代码段通过网络运行音频（保存在wav文件中）。\n\n可以使用你的模型对新的音频片段进行预测。\n\n你首先需要计算输入音频剪辑的预测。\n\n**练习**：实现predict_activates（）。你需要执行以下操作：\n\n1. 计算音频文件的频谱图\n1. 使用`np.swap`和`np.expand_dims`将输入调整为（1，Tx，n_freqs）大小\n1. 在模型上使用正向传播来计算每个输出步骤的预测"},{"cell_type":"code","execution_count":41,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"3B0E33DB77434E1EAC626E0924AF0B31","jupyter":{},"scrolled":false},"outputs":[],"source":"def detect_triggerword(filename):\n    plt.subplot(2, 1, 1)\n\n    x = graph_spectrogram(filename)\n    # the spectogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model\n    x  = x.swapaxes(0,1)\n    x = np.expand_dims(x, axis=0)\n    predictions = model.predict(x)\n    \n    plt.subplot(2, 1, 2)\n    plt.plot(predictions[0,:,0])\n    plt.ylabel('probability')\n    plt.show()\n    return predictions"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"A1FCE3C33BFA42AB95660E67FD9570EB","jupyter":{},"mdEditEnable":false},"source":"一旦估计了在每个输出步骤中检测到\"activate\"一词的可能性，就可以在该可能性高于某个阈值时触发出\"chiming（蜂鸣）\"声。此外，在说出\"activate\"之后，对于许多连续值，$y^{\\langle t \\rangle}$可能接近1，但我们只希望发出一次提示音。因此，每75个输出步骤最多将插入一次铃声。这将有助于防止我们为\"activate\"的单个实例插入两个提示音。（该作用类似于计算机视觉中的非极大值抑制）\n\n**练习**：实现chime_on_activate（）。你需要执行以下操作：\n\n1.遍历每个输出步骤的预测概率\n2.当预测大于阈值并且经过了连续75个以上的时间步长时，在原始音频剪辑中插入\"chime\"\n\n使用以下代码将1375步离散化转换为10000步离散化，并使用pydub插入“chime”：\n` audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio.duration_seconds)*1000)\n`\n"},{"cell_type":"code","execution_count":42,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"065DF5DCDCD143919DCCFB477D320489","jupyter":{},"scrolled":false},"outputs":[],"source":"chime_file = \"audio_examples/chime.wav\"\ndef chime_on_activate(filename, predictions, threshold):\n    audio_clip = AudioSegment.from_wav(filename)\n    chime = AudioSegment.from_wav(chime_file)\n    Ty = predictions.shape[1]\n    # Step 1: Initialize the number of consecutive output steps to 0\n    consecutive_timesteps = 0\n    # Step 2: Loop over the output steps in the y\n    for i in range(Ty):\n        # Step 3: Increment consecutive output steps\n        consecutive_timesteps += 1\n        # Step 4: If prediction is higher than the threshold and more than 75 consecutive output steps have passed\n        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:\n            # Step 5: Superpose audio and background using pydub\n            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)\n            # Step 6: Reset consecutive output steps to 0\n            consecutive_timesteps = 0\n        \n    audio_clip.export(\"chime_output.wav\", format='wav')"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"8A32F6D52B524F8983893625216CD4D1","jupyter":{},"mdEditEnable":false},"source":"## 3.1 测试开发集"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"CDA359B5F1E6455685C859AD6EE637BE","jupyter":{},"mdEditEnable":false},"source":"让我们探讨一下我们的模型在开发集中的两个未知的音频剪辑上表现如何。首先让我们听听两个开发集剪辑。"},{"cell_type":"code","execution_count":43,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"48AF413189CD4CE7916F38AE5DC5A503","jupyter":{},"scrolled":false},"outputs":[],"source":"IPython.display.Audio(\"./raw_data/dev/1.wav\")"},{"cell_type":"code","execution_count":44,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"35E2C3089C5F4958924FCE0D6C8FAC7E","jupyter":{},"scrolled":false},"outputs":[],"source":"IPython.display.Audio(\"./raw_data/dev/2.wav\")"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"4E3561D55616467F8BABBEFB411BFF61","jupyter":{},"mdEditEnable":false},"source":"现在，让我们在这些音频剪辑上运行模型，看看在\"activate\"之后它是否添加了提示音！"},{"cell_type":"code","execution_count":45,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"C450C9FD53A4485C903C58EAB4DCEB72","jupyter":{},"scrolled":false},"outputs":[],"source":"filename = \"./raw_data/dev/1.wav\"\nprediction = detect_triggerword(filename)\nchime_on_activate(filename, prediction, 0.5)\nIPython.display.Audio(\"./chime_output.wav\")"},{"cell_type":"code","execution_count":46,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"8E9FE0E88E8148F0922073A8A8E8D170","jupyter":{},"scrolled":false},"outputs":[],"source":"filename  = \"./raw_data/dev/2.wav\"\nprediction = detect_triggerword(filename)\nchime_on_activate(filename, prediction, 0.5)\nIPython.display.Audio(\"./chime_output.wav\")"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"88D986F02A5C4BFA8258A87DAEBA3F13","jupyter":{},"mdEditEnable":false},"source":"恭喜，你已结束本作业！\n\n这是你应该记住的：\n- 数据合成是创建针对语音问题（尤其是触发词检测）大型训练集的有效方法。\n- 在将音频数据传递到RNN，GRU或LSTM之前，使用频谱图和可选的1D转换层是常见的预处理步骤。\n- 可以使用端到端的深度学习方法来构建非常有效的触发词检测系统。\n\n*恭喜*完成最后的作业！\n\n感谢你一直坚持到底，并感谢你为学习深度学习付出的辛勤工作。希望你喜欢这个课程！\n"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"C89228AE0F9544948F6E5AD73BB5C8D6","jupyter":{},"mdEditEnable":false},"source":"# 4 试试你自己的例子！（可选练习）\n\n在此笔记本的此可选练习中，你可以在自己的音频剪辑上尝试使用你的模型！\n\n录制一个10秒钟的音频片段，说\"activate\"和其他随机单词，然后将其作为`myaudio.wav`上传到Coursera hub。确保将音频作为WAV文件上传。如果你的音频以其他格式（例如mp3）录制，则可以在线找到免费软件以将其转换为wav。如果你的录音时间不是10秒，则下面的代码将根据需要修剪或填充该声音，以使其达到10秒。\n"},{"cell_type":"code","execution_count":47,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"449C9E0DCA6E4ED887616CE7B4256E19","jupyter":{},"scrolled":false},"outputs":[],"source":"# Preprocess the audio to the correct format\ndef preprocess_audio(filename):\n    # Trim or pad audio segment to 10000ms\n    padding = AudioSegment.silent(duration=10000)\n    segment = AudioSegment.from_wav(filename)[:10000]\n    segment = padding.overlay(segment)\n    # Set frame rate to 44100\n    segment = segment.set_frame_rate(44100)\n    # Export as wav\n    segment.export(filename, format='wav')"},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"76B04195047E4755A18B01DB15EF2C25","jupyter":{},"mdEditEnable":false},"source":"将音频文件上传到Coursera后，请在下面的变量中输入文件的路径。"},{"cell_type":"code","execution_count":48,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"D11BB7C98BD14F0A809F670E3BDC8B69","jupyter":{},"scrolled":false},"outputs":[],"source":"your_filename = \"audio_examples/my_audio.wav\""},{"cell_type":"code","execution_count":49,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"C12A9A6020FB4BF78554C9BE42D6FBEB","jupyter":{},"scrolled":false},"outputs":[],"source":"preprocess_audio(your_filename)\nIPython.display.Audio(your_filename) # listen to the audio you uploaded "},{"cell_type":"markdown","metadata":{"tags":[],"slideshow":{"slide_type":"slide"},"id":"860B6D00A178415AB79DA7F837B0FF8F","jupyter":{},"mdEditEnable":false},"source":"最后，使用该模型预测在10秒的音频剪辑中何时说了\"activate\"并触发提示音。如果没有适当添加哔声，请尝试调整chime_threshold。"},{"cell_type":"code","execution_count":50,"metadata":{"collapsed":false,"tags":[],"slideshow":{"slide_type":"slide"},"id":"F6E4CDCF861C4A069E93452497D30D36","jupyter":{},"scrolled":false},"outputs":[],"source":"chime_threshold = 0.5\nprediction = detect_triggerword(your_filename)\nchime_on_activate(your_filename, prediction, chime_threshold)\nIPython.display.Audio(\"./chime_output.wav\")"},{"metadata":{"id":"DFD8D71B31554FD68123689423D3E96A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}