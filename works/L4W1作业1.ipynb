{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7864BF5BB76442FFBE6D62FA46B2A7F7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 逐步实现卷积神经网络\n",
    "\n",
    "欢迎来到课程4的第一个作业！在此作业中，你将使用numpy实现卷积（CONV）和池化（POOL）层，包括正向传播和反向传播（可选）。\n",
    "\n",
    "**符号**：\n",
    "- 上标$[l]$表示第$l^{th}$层的对象。\n",
    "    - 例如：$a^{[4]}$是$4^{th}$层的激活。 $W^{[5]}$和$b^{[5]}$是$5^{th}$层的参数。\n",
    "\n",
    "- 上标$(i)$表示第$i^{th}$个示例中的对象。\n",
    "    - 示例：$x^{(i)}$是$i^{th}$个训练数据的输入。\n",
    "       \n",
    "- 下标$i$表示$i^{th}$的向量输入。\n",
    "    - 示例：$a^{[l]}_i$表示$l$层中的$i^{th}$个激活，假设这是全连接层（FC）。\n",
    "    \n",
    "    \n",
    "- $n_H$, $n_W$和$n_C$分别表示给定层的通道的高度，宽度和数量。如果要引用特定层$l$，则还可以写入 $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$。\n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$和$n_{C_{prev}}$ 分别表示前一层的高度，宽度和通道数。如果引用特定层$l$，则也可以表示为$n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$。\n",
    "\n",
    "我们假设你已经熟悉`numpy`或者已经完成了之前的专业课程。那就开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5098E74F09CE41C88B1E364EDCCF0E7F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1 安装包\n",
    "\n",
    "让我们首先导入在作业过程中需要用到的包：\n",
    "- [numpy](www.numpy.org) 是Python科学计算的基本包。\n",
    "- [matplotlib](http://matplotlib.org) 是在Python中常用的绘制图形的库。\n",
    "- np.random.seed（1）使所有随机函数调用保持一致。这将帮助我们为你的作品评分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "101696E78CD648478B28587FF364FDFD",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\deep learning\\deeplearning.ai\\data\\L4W1\n"
     ]
    }
   ],
   "source": [
    "cd ../data/L4W1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "59757F282B58470D8902E42F0B51E712",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46BBD45B13DA458693DFB0BF214B7DF1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2 作业大纲\n",
    "\n",
    "你将实现构建卷积神经网络的需要的模块！要求实现的每个函数都有详细的说明，以帮助你完成所需的步骤：\n",
    "\n",
    "- 卷积函数，包括：\n",
    "    - 零填充\n",
    "    - 卷积窗口\n",
    "    - 正向卷积\n",
    "    - 反向卷积（可选）\n",
    "- 池化函数，包括：\n",
    "    - 正向池化\n",
    "    - 创建mask\n",
    "    - 分配值\n",
    "    - 反向池化（可选）\n",
    "    \n",
    "本笔记本将要求你使用 `numpy`从头开始实现这些函数。在下一本笔记本中，你将学习使用TensorFlow来实现：\n",
    "\n",
    "![Image](../image/L4W1/1.png)\n",
    "\n",
    "**注意**，对于每个正向函数，都有其对应的反向等式。因此，在正向传播模块的每一步中，都将一些参数存储在缓存中。这些参数用于在反向传播时计算梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DD5D5B507432448483CC23A089070526",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3 卷积神经网络\n",
    "\n",
    "尽管编程框架可以方便使用卷积，但它们仍然是深度学习中最难理解的概念之一。卷积层将输入体积转换为不同大小的输出体积，如下所示。\n",
    "\n",
    "![Image](../image/L4W1/2.png)\n",
    "\n",
    "\n",
    "在这一部分，你将构建卷积层的每一步。首先实现两个辅助函数：一个用于零填充，另一个用于计算卷积函数本身。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80A86D65A11043AA851BCA1ADD73965F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3.1 零填充\n",
    "\n",
    "零填充将在图像的边界周围添加零：\n",
    "\n",
    "![Image](../image/L4W1/3.png)\n",
    "\n",
    "**图1 **：**零填充**\n",
    "图像（3个通道，RGB），填充2次。\n",
    "\n",
    "填充的主要好处有：\n",
    "- 允许使用CONV层而不必缩小其高度和宽度。这对于构建更深的网络很重要，因为高度/宽度会随着更深的层而缩小。一个重要、特殊的例子是\"same\"卷积，其中高度/宽度在一层之后被精确保留。\n",
    "- 有助于我们将更多信息保留在图像边缘。如果不进行填充，下一层的一部分值将会受到图像边缘像素的干扰。\n",
    "\n",
    "**练习**：实现以下函数，该功能将使用零填充处理一个批次X的所有图像数据。[Use np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html)。注意，如果要填充维度为$(5,5,5,5,5)$的数组“a”，则第二维的填充为`pad = 1`，第四维的填充为`pad = 3`，其余为`pad = 0`，你可以这样做：\n",
    "```python\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "C399C94BC39945E98B04981348718618",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: zero_pad\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    X_pad = np.pad(X, ((0, 0),(pad, pad),(pad, pad),(0, 0)), 'constant', constant_values=0)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2215E4EADF084E618A038C50A828014D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_pad.shape = (4, 7, 7, 2)\n",
      "x[1,1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] = [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x22d30e79788>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1,1] =\", x[1,1])\n",
    "print (\"x_pad[1,1] =\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A709953D93874369885ADEC4324F54B8",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**:\n",
    "x.shape = (4, 3, 3, 2)\n",
    "x_pad.shape = (4, 7, 7, 2)\n",
    "x[1,1] = [[ 0.90085595 -0.68372786]\n",
    " [-0.12289023 -0.93576943]\n",
    " [-0.26788808  0.53035547]]\n",
    "x_pad[1,1] = [[0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]\n",
    " [0. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFFD353E56CA47F68F8B1FFE8294A8C6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3.2 卷积的单个步骤\n",
    "\n",
    "在这一部分中，实现卷积的单个步骤，其中将滤波器（卷积核）应用于输入的单个位置。这将用于构建卷积单元，该卷积单元：\n",
    "\n",
    "- 占用输入体积\n",
    "- 在输入的每个位置都应用滤波器\n",
    "- 输出另一个体积（通常大小不同）\n",
    "\n",
    "![Image](../image/L4W1/4.gif)\n",
    "\n",
    "\n",
    "**图2 **：**卷积操作**\n",
    "滤波器大小为2x2，步幅为1（步幅=每次滑动时移动窗口的数量）\n",
    "\n",
    "在计算机视觉应用中，左侧矩阵中的每个值都对应一个像素值，我们将3x3滤波器与图像进行卷积操作，首先将滤波器元素的值与原始矩阵相乘，然后将它们相加。在练习的第一步中，你将实现卷积的单个步骤，相当于仅对一个位置应用滤波器以获得单个实值输出。\n",
    "\n",
    "在本笔记本的后面，你将应用此函数于输入的多个位置以实现完整的卷积运算。\n",
    "\n",
    "**练习**：实现conv_single_step()。 [提示](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "B0DD017F1B204504BD427989D5E19F96",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_single_step\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Element-wise product between a_slice and W. Add bias.\n",
    "    s = np.multiply(a_slice_prev, W) + b\n",
    "    # Sum over all entries of the volume s\n",
    "    Z = np.sum(s)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "45B950C6E36D4D3084ADE0DB981A8A4E",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -23.16021220252078\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0C873B798C27468188B49E2543C032A9",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**:\n",
    "Z = -23.16021220252078"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "CAA659AC2777404EB7D33CDCEB238FA7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### 3.3 卷积神经网络--正向传递\n",
    "\n",
    "在正向传递中，你将使用多个滤波器对输入进行卷积。每个“卷积”都会输出一个2D矩阵。然后，你将堆叠这些输出以获得3：\n",
    "\n",
    "![Image](../image/L4W1/5.gif)\n",
    "\n",
    "**练习**：实现以下函数，使用滤波器W卷积输入A_prev。此函数将上一层的激活输出（对于一批m个输入）A_prev作为输入，F表示滤波器/权重（W）和偏置向量（b），其中每个滤波器都有自己的（单个）偏置。最后，你还可以访问包含stride和padding的超参数字典。\n",
    "\n",
    "**提示**：\n",
    "1.要在矩阵“a_prev”（5,5,3）的左上角选择一个2x2切片，请执行以下操作：\n",
    "```python\n",
    "a_slice_prev = a_prev[0:2,0:2,:]\n",
    "```\n",
    "使用定义的`start/end`索引定义`a_slice_prev`时将非常有用。\n",
    "2.要定义a_slice，你需要首先定义其角点 `vert_start`, `vert_end`, `horiz_start` 和 `horiz_end`。该图可能有助于你找到如何在下面的代码中使用h，w，f和s定义每个角。\n",
    "\n",
    "![Image](../image/L4W1/6.png)\n",
    "\n",
    "**图3 **：**使用垂直和水平的start/end（2x2滤波器）定义切片**\n",
    "\n",
    "该图仅显示一个通道。\n",
    "\n",
    "**提醒**：\n",
    "卷积的输出维度与输入维度相关公式为：\n",
    "$$\n",
    "n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1\n",
    "$$\n",
    "$$\n",
    "n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1\n",
    "$$\n",
    "$$\n",
    "n_C = \\text{number of filters used in the convolution}\n",
    "$$\n",
    "\n",
    "对于此作业，我们不必考虑向量化，只使用for循环实现所有函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "560FE46D7311453894AB85F6095FAF9C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_forward\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = 1 + int((n_H_prev + 2 * pad - f) / stride)\n",
    "    n_W = 1 + int((n_W_prev + 2 * pad - f) / stride)\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i]                               # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                  \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    Z[i, h, w, c] = np.sum(np.multiply(a_slice_prev, W[:, :, :, c]) + b[:, :, :, c])\n",
    "                                        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "39DA53AC7421479294CB41A1DC06033F",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prev = (4, 4, 3) * 10\n",
      "pad = 2\n",
      "A_prev_pad = [8 8 3] * 10\n",
      "mask = (2, 2, 3) * 8\n",
      "stride = 1\n",
      "Z = (7, 7, 8) * 10\n",
      "Z's mean = 0.15585932488906465\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 1}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "print(\"A_prev =\", A_prev.shape[1:], \"*\", A_prev.shape[0])\n",
    "print(\"pad =\", hparameters['pad'])\n",
    "print(\"A_prev_pad =\", A_prev.shape[1:] + np.array([hparameters['pad']*2,hparameters['pad']*2,0]), \"*\", A_prev.shape[0])\n",
    "print(\"mask =\", W.shape[:3], \"*\", W.shape[3])\n",
    "print(\"stride =\", hparameters['stride'])\n",
    "print(\"Z =\", Z.shape[1:], \"*\", Z.shape[0])\n",
    "\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5F714AC5AACE45A292DF32BE42B2E776",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**:\n",
    "Z's mean = 0.15585932488906465\n",
    "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5C529946F6DD4F528E001AAEC7B20759",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "最后，CONV层还应包含一个激活，此情况下，我们将添加以下代码行：\n",
    "\n",
    "```python\n",
    "# Convolve the window to get back one output neuron\n",
    "Z[i, h, w, c] = ...\n",
    "# Apply activation\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])\n",
    "```\n",
    "\n",
    "在这里你不需要做这个。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8C61E3AF68F84AAA819C5D962970EB8E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 4 池化层\n",
    "\n",
    "池化（POOL）层减少了输入的高度和宽度。它有助于减少计算量，而且可以使特征检测器在输入中的位置保持不变。池化层有两种：\n",
    "\n",
    "- 最大池化：在输入上滑动 ($f, f$)窗口，并将窗口的最大值存储在输出中。\n",
    "\n",
    "- 平均池化：在输入上滑动 ($f, f$)窗口，并将该窗口的平均值存储在输出中。\n",
    "\n",
    "![Image](../image/L4W1/7.png)\n",
    "\n",
    "![Image](../image/L4W1/8.png)\n",
    "\n",
    "这些池化层没有用于反向传播训练的参数。但是，它们具有超参数，例如窗口大小$f$，它指定了你要计算最大值或平均值的窗口的高度和宽度。\n",
    "\n",
    "### 4.1 正向池化\n",
    "现在，你将在同一函数中实现最大池化和平均池化。\n",
    "\n",
    "**练习**：实现池化层的正向传播。请遵循下述提示。\n",
    "\n",
    "**提示**：\n",
    "由于没有填充，因此将池化的输出维度绑定到输入维度的公式为：\n",
    "$$\n",
    "n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1\n",
    "$$\n",
    "$$\n",
    "n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1\n",
    "$$\n",
    "$$\n",
    "n_C = n_{C_{prev}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "0CEB88A7762E42748561C503B1A665C8",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pool_forward\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "195B2E598A574FD1912520CE239929CF",
    "jupyter": {},
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prev = (4, 4, 3) * 2\n",
      "mode = max\n",
      "A = (1, 1, 3) * 2\n",
      "mode = average\n",
      "A = (1, 1, 3) * 2\n",
      "[[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]\n",
      " [ 1.74481176 -0.7612069   0.3190391 ]\n",
      " [-0.24937038  1.46210794 -2.06014071]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 4}\n",
    "\n",
    "\n",
    "print(\"A_prev =\", A_prev.shape[1:], \"*\", A_prev.shape[0])\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "\n",
    "print(\"A =\", A.shape[1:],\"*\",A.shape[0])\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A.shape[1:],\"*\",A.shape[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDBE42E0CDCF407B8B6DBA5992F3BF07",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出:**\n",
    "mode = max\n",
    "A = [[[[1.74481176 1.6924546  2.10025514]]]\n",
    "\n",
    "\n",
    " [[[1.19891788 1.51981682 2.18557541]]]]\n",
    "\n",
    "mode = average\n",
    "A = [[[[-0.09498456  0.11180064 -0.14263511]]]\n",
    "\n",
    "\n",
    " [[[-0.09525108  0.28325018  0.33035185]]]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "828CA5D3378345798185183E36F0DF50",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "Nice！现在你已经实现了卷积网络所有层的正向传播。\n",
    "\n",
    "笔记本的其余部分是可选择学习的，不会用于评分。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D2DE34E289F465285F31133258E1305",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 5 卷积神经网络中的反向传播（可选练习）\n",
    "\n",
    "在深度学习框架中，你只需要实现正向传播，该框架就可以处理反向传播，因此大多数深度学习工程师不需要理会反向传播的细节。卷积网络的反向传播很复杂。但是，如果你愿意，可以在笔记本的此可选部分中进行操作，以了解卷积网络中反向传播的原理。\n",
    "\n",
    "在较早的课程中，当你实现了一个简单的（全连接）神经网络时，你就使用了反向传播来计算损失的导数以更新参数。类似地，在卷积神经网络中，你可以计算损失的导数以更新参数。反向传播方程并非不重要，即使我们在课程中并未导出它们，但下面简要介绍了过程。\n",
    "\n",
    "### 5.1 卷积层的反向传播\n",
    "\n",
    "让我们从实现CONV层的反向传播开始。\n",
    "\n",
    "#### 5.1.1 计算 dA：\n",
    "这是用于针对特定滤波器$W_c$的损失和给定训练示例计算$dA$的公式：\n",
    "\n",
    "$$\n",
    "dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}\n",
    "$$\n",
    "\n",
    "其中$W_c$ 是一个滤波器，$dZ_{hw}$是一个标量，相对于第h行和第w列的conv层Z的输出的梯度的损失。请注意，每次更新dA时，我们都会将相同的滤波器$W_c$ 乘以不同的dZ。我们这样做主要是因为在计算正向传播时，每个滤波器都由不同的a_slice进行点乘和求和。因此，在为dA计算backprop时，我们只是加上所有a_slices的梯度。\n",
    "\n",
    "在适当的for循环内，此公式转换为：\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.2 计算 dW：\n",
    "这是用于针对损失计算$dW_c$的公式（$dW_c$是一个滤波器的导数）：\n",
    "\n",
    "$$\n",
    "dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}\n",
    "$$\n",
    "\n",
    "其中$a_{slice}$对应于用于生成激活$Z_{ij}$的切片，最终我们得到$W$相对于该切片的梯度。由于它是相同的$W$，因此我们将所有这些梯度加起来即可得到$dW$。\n",
    "\n",
    "在适当的for循环内，此公式转换为：\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.3 计算 db：\n",
    "\n",
    "这是用于某个滤波器$W_c$的损失计算$db$的公式：\n",
    "\n",
    "$$\n",
    "db = \\ sum_h \\ sum_w dZ_ {hw} \\ tag {3}\n",
    "$$\n",
    "\n",
    "正如你先前在基本神经网络中所见，db是通过将$dZ$相加得出的。在这种情况下，你只需要对转换输出（Z）相对于损失的所有梯度求和。\n",
    "\n",
    "在适当的for循环内，此公式转换为：\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "**练习**：在下面实现`conv_backward`函数。你应该总结所有训练数据，滤波器，高度和宽度。然后，你应该使用上面的公式1、2和3计算导数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "D645F70188E94AF3911304D381399BF1",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "43704C81892440128A012B6404CE1C4A",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.608990675868995\n",
      "dW_mean = 10.581741275547566\n",
      "db_mean = 76.37106919563735\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "433AB423ED6C4EB399E3055FE53F1C22",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出: **\n",
    "dA_mean = 9.608990675868995\n",
    "dW_mean = 10.581741275547563\n",
    "db_mean = 76.37106919563735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CFAB43CC04D049F38A0B3445738CB9E2",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 5.2 池化层--反向传播\n",
    "\n",
    "接下来，让我们从MAX-POOL层开始实现池化层的反向传播。即使池化层没有用于反向传播更新的参数，你仍需要通过池化层对梯度进行反向传播，以便计算池化层之前的层的梯度。\n",
    "\n",
    "#### 5.2.1 最大池化--反向传播\n",
    "\n",
    "在进入池化层的反向传播之前，首先构建一个名为`create_mask_from_window()`的辅助函数，该函数将执行以下操作：\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}\n",
    "$$\n",
    "\n",
    "此函数创建一个“掩码”矩阵，该矩阵追踪矩阵的最大值。True（1）表示最大值在X中的位置，其他条目为False（0）。稍后你将看到，平均池的反向传播与此相似，但是使用了不同的掩码。\n",
    "\n",
    "**练习**：实现`create_mask_from_window（）`。此函数将有助于反向池化。\n",
    "**提示：**\n",
    "- [np.max（）]（）可能会有所帮助。它计算一个数组的最大值。\n",
    "- 如果有一个矩阵X和一个标量x：`A =（X == x）`将返回与X大小相同的矩阵A，从而：\n",
    "```\n",
    "A[i,j] = True if X[i,j] = x\n",
    "A[i,j] = False if X[i,j] != x\n",
    "```\n",
    "- 此处无需考虑矩阵中有多个最大值的情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "8AA821ACBEC048A18A180D9BF3E88DE0",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = (x == np.max(x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "CC2D31AAEE754B15BD7C481FD672774C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "4E2E18FFC01E419C8B73CA65F11698AC",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出:** \n",
    "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
    " [-1.07296862  0.86540763 -2.3015387 ]]\n",
    "mask =  [[ True False False]\n",
    " [False False False]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B3967673E524F5A8E03514EDB30F946",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "为什么我们要追踪最大值的位置？因为这是最终影响输出的输入值，也影响了损失。 反向传播算法是根据损失计算梯度的，因此影响最终损失的任何事物都应具有非零的梯度。因此，反向传播将使梯度“传播”回影响损失的特定输入值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B121758D2E614C588BA46079C29E2317",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### 5.2.2 平均池化--反向传播\n",
    "\n",
    "在最大池化中，对于每个输入窗口，输出上的所有“影响”都来自单个输入值，即最大值。在平均池化中，输入窗口的每个元素对输出的影响均相同 因此，要实现反向传播，你现在将实现一个反映此点的辅助函数。\n",
    "\n",
    "例如，如果我们使用2x2滤波器在正向传播中进行平均池化，那么用于反向传播的掩码将如下所示：\n",
    "$$\n",
    "dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}\n",
    "$$\n",
    "\n",
    "这意味着矩阵$dZ$中的每个位置对输出的贡献均等，因为在正向传播中，我们取平均值。\n",
    "\n",
    "**练习**：实现以下函数，以通过维度矩阵平均分配值dz。 [提示](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ones.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "98E380512A46486684212BFA3B12B24E",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "05602ACF7AD3405BB191AC47F1B03181",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FA04A03D1304C33B6C9C38B1CD2FF54",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**: \n",
    "distributed value = [[0.5 0.5]\n",
    " [0.5 0.5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABD33DC614AC403B9E5F30C4E3B710F3",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#### 5.2.3 组合：反向池化\n",
    "\n",
    "现在，你准备好了在池化层上计算反向传播所需的一切。\n",
    "\n",
    "**练习**：在两种模式（`\"max\"`和`\"average\"`）都实现“pool_backward”功能。再次使用4个for循环（遍历训练数据，高度，宽度和通道）。使用 `if/elif`语句来查看模式是否等于`'max'`或`'average'`。如果等于'average' ，则应使用上面实现的`distribute_value()`函数创建与 `a_slice`维度相同的矩阵。此外，模式等于'`max`'时，你将使用 `create_mask_from_window()`创建一个掩码，并将其乘以相应的dZ值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "F103C9F044B143DD87D417B62E42E79D",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    f = hparameters['f']\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros_like(A_prev)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i, vert_start, horiz_start, c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, vert_start, horiz_start, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "1878278E656A4E83BB26A763C1C50EAA",
    "jupyter": {},
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66FB2A67009F40CC84C0327E4C773ECB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**: \n",
    "mode = max\n",
    "mean of dA =  0.14571390272918056\n",
    "dA_prev[1,1] =  [[ 0.          0.        ]\n",
    " [ 5.05844394 -1.68282702]\n",
    " [ 0.          0.        ]]\n",
    "\n",
    "mode = average\n",
    "mean of dA =  0.14571390272918056\n",
    "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
    " [ 1.26461098 -0.25749373]\n",
    " [ 1.17975636 -0.53624893]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CE704B5978E4DBF80B317A263484063",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 恭喜！\n",
    "\n",
    "祝贺你完成此作业。你现在了解了卷积神经网络如何工作，实现了构建神经网络所需的所有模块。\n",
    "在下一项作业中，你将使用TensorFlow实现ConvNet。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machineLearing]",
   "language": "python",
   "name": "conda-env-machineLearing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
