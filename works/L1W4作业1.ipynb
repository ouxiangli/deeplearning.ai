{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A36728B5401D4D968741C591CA2E598C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 逐步构建你的深度神经网络\n",
    "\n",
    "欢迎来到第4周作业的第1部分！在此之前你已经训练了一个2层的神经网络（只有一个隐藏层）。本周，你将学会构建一个任意层数的深度神经网络！\n",
    "\n",
    "- 在此作业中，你将实现构建深度神经网络所需的所有函数。\n",
    "- 在下一个作业中，你将使用这些函数来构建一个用于图像分类的深度神经网络。\n",
    "\n",
    "**完成此任务后，你将能够：**\n",
    "- 使用ReLU等非线性单位来改善模型\n",
    "- 建立更深的神经网络（具有1个以上的隐藏层）\n",
    "- 实现一个易于使用的神经网络类\n",
    "\n",
    "**符号说明**：\n",
    "- 上标$[l]$ 表示与$l^{th}$ 层相关的数量。\n",
    "    - 示例：$a^{[L]}$ 是$L^{th}$ 层的激活。 $W^{[L]}$ 和$b^{[L]}$是$L^{th}$层参数。\n",
    "- 上标$(i)$ 表示与$i^{th}$示例相关的数量。\n",
    "    - 示例：$x^{(i)}$是第$i^{th}$ 的训练数据。\n",
    "- 下标$i$ 表示$i^{th}$的向量。\n",
    "    - 示例：$a^{[l]}_i$ 表示$l^{th}$ 层激活的$i^{th}$ 输入。\n",
    "\n",
    "让我们开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2C5D9E8C60D242DE8F5CD26F8539F75C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1 安装包\n",
    "\n",
    "让我们首先导入作业过程中需要的所有包。\n",
    "- [numpy](www.numpy.org)是Python科学计算的基本包。\n",
    "- [matplotlib](http://matplotlib.org)是在Python中常用的绘制图形的库。\n",
    "- dnn_utils为此笔记本提供了一些必要的函数。\n",
    "- testCases提供了一些测试用例来评估函数的正确性\n",
    "- np.random.seed（1）使所有随机函数调用保持一致。 这将有助于我们评估你的作业，请不要改变seed。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4A8B5FDBDA22482A8069D8CA81B123B4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\deep learning\\deeplearning.ai\\data\\L1W4\n"
     ]
    }
   ],
   "source": [
    "cd ../data/L1W4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "FBECBCF39C704FBFAF24435091108C59",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v2 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "0700150647BE41959EC95C9A3B5D2393",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2 作业大纲\n",
    "\n",
    "为了构建你的神经网络，你将实现几个“辅助函数”。这些辅助函数将在下一个作业中使用，用来构建一个两层神经网络和一个L层的神经网络。你将实现的每个函数都有详细的说明，这些说明将指导你完成必要的步骤。此作业的大纲如下：\n",
    "\n",
    "- 初始化两层的网络和$L$层的神经网络的参数。\n",
    "- 实现正向传播模块（在下图中以紫色显示）。\n",
    "     - 完成模型正向传播步骤的LINEAR部分（$Z^{[l]}$）。\n",
    "     - 提供使用的ACTIVATION函数（relu / Sigmoid）。\n",
    "     - 将前两个步骤合并为新的[LINEAR-> ACTIVATION]前向函数。\n",
    "     - 堆叠[LINEAR-> RELU]正向函数L-1次（第1到L-1层），并在末尾添加[LINEAR-> SIGMOID]（最后的$L$层）。这合成了一个新的L_model_forward函数。\n",
    "- 计算损失。\n",
    "- 实现反向传播模块（在下图中以红色表示）。\n",
    "    - 完成模型反向传播步骤的LINEAR部分。\n",
    "    - 提供的ACTIVATE函数的梯度（relu_backward / sigmoid_backward）\n",
    "    - 将前两个步骤组合成新的[LINEAR-> ACTIVATION]反向函数。\n",
    "    - 将[LINEAR-> RELU]向后堆叠L-1次，并在新的L_model_backward函数中后向添加[LINEAR-> SIGMOID]\n",
    "- 最后更新参数。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q1d3at209q.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "**图1**\n",
    "\n",
    "**注意**：对于每个正向函数，都有一个对应的反向函数。 这也是为什么在正向传播模块的每一步都将一些值存储在缓存中的原因。缓存的值可用于计算梯度。 然后，在反向传导模块中，你将使用缓存的值来计算梯度。 此作业将指导说明如何执行这些步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "459E3033A0974A11801126A53F5C5CB7",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3 初始化\n",
    "\n",
    "首先编写两个辅助函数用来初始化模型的参数。 第一个函数将用于初始化两层模型的参数。 第二个将把初始化过程推广到$L$层模型上。\n",
    "\n",
    "### 3.1 2层神经网络\n",
    "\n",
    "**练习**：创建并初始化2层神经网络的参数。\n",
    "\n",
    "**说明**：\n",
    "- 模型的结构为：*LINEAR -> RELU -> LINEAR -> SIGMOID*。\n",
    "- 随机初始化权重矩阵。 确保准确的维度，使用`np.random.randn（shape）* 0.01`。\n",
    "- 将偏差初始化为0。 使用`np.zeros（shape）`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "A6B5BB2077CC47E78E37DEAD0A40E018",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F8690551C3D24958B2E41EFB2CB8E016",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756]\n",
      " [-0.00528172 -0.01072969]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.00865408 -0.02301539]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(2,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3D2FF063646B4A598428A1140D4E7519",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**:\n",
    "W1 = [[ 0.01624345 -0.00611756]\n",
    " [-0.00528172 -0.01072969]]\n",
    "b1 = [[0.]\n",
    " [0.]]\n",
    "W2 = [[ 0.00865408 -0.02301539]]\n",
    "b2 = [[0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "0C265B19D4FF4F9C9FECB268B1152D7D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3.2 L层神经网络\n",
    "\n",
    "更深的L层神经网络的初始化更加复杂，因为存在更多的权重矩阵和偏差向量。 完成 `initialize_parameters_deep`后，应确保各层之间的维度匹配。 回想一下，$n^{[l]}$是$l$层中的神经元数量。 因此，如果我们输入的 $X$ 的大小为$(12288, 209)$（以$m=209$为例），则：\n",
    "\n",
    "\n",
    "|           | Shape of W | Shape of b | Activation | Shape of  Activation |\n",
    "| --------- | ---------- | ---------- | ---------- | -------------------- |\n",
    "| Layer 1   | $(n^{[1]},12288)$ | $(n^{[1]},1)$ | $Z^{[1]} = W^{[1]}  X + b^{[1]} $ | $(n^{[1]},209)$ |\n",
    "| Layer 2   | $(n^{[2]}, n^{[1]})$ | $(n^{[2]},1)$ | $Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ | $(n^{[2]}, 209)$ |\n",
    "| $\\vdots$| $\\vdots$ | $\\vdots$ | $\\vdots$ | $\\vdots$ |\n",
    "| Layer L-1 | $(n^{[L-1]}, n^{[L-2]})$ | $(n^{[L-1]}, 1)$ | $Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ | $(n^{[L-1]}, 209)$ |\n",
    "| Layer L   | $(n^{[L]}, n^{[L-1]})$ | $(n^{[L]}, 1)$ | $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$ | $(n^{[L]}, 209)$ |\n",
    "\n",
    "\n",
    "当我们在python中计算$W X + b$时，使用广播，比如：\n",
    "\n",
    "$$ \n",
    "W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r  \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}\n",
    "$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$ \n",
    "WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5691608A86384B1C822EDAFCBF06D3D8",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**练习**：实现L层神经网络的初始化。\n",
    "\n",
    "**说明**：\n",
    "- 模型的结构为 *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*。也就是说，$L-1$层使用ReLU作为激活函数，最后一层采用sigmoid激活函数输出。\n",
    "- 随机初始化权重矩阵。使用`np.random.rand（shape）* 0.01`。\n",
    "- 零初始化偏差。使用`np.zeros（shape）`。\n",
    "- 我们将在不同的layer_dims变量中存储$n^{[l]}$，即不同层中的神经元数。例如，上周“二维数据分类模型”的`layer_dims`为[2,4,1]：即有两个输入，一个隐藏层包含4个隐藏单元，一个输出层包含1个输出单元。因此，`W1`的维度为（4,2），`b1`的维度为（4,1），`W2`的维度为（1,4），而`b2`的维度为（1,1）。现在你将把它应用到$L$层！\n",
    "- 这是$L=1$ （一层神经网络）的实现。以启发你如何实现通用的神经网络（L层神经网络）。\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4F8206C746D2414A899A1B4E61D6B117",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "DFA07513D4144D27ADA96D7FB489C784",
    "jupyter": {},
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92CBBCA67CA04B6B84E473BAC4206383",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**:\n",
    "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
    "b1 = [[0.]\n",
    " [0.]\n",
    " [0.]\n",
    " [0.]]\n",
    "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
    "b2 = [[0.]\n",
    " [0.]\n",
    " [0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB865BBF6ADF4F9A8455579A2A37EE21",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 4 正向传播模块\n",
    "\n",
    "### 4.1 线性正向\n",
    "现在，你已经初始化了参数，接下来将执行正向传播模块。 首先实现一些基本函数，用于稍后的模型实现。按以下顺序完成三个函数：\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION，其中激活函数采用ReLU或Sigmoid。\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID（整个模型）\n",
    "\n",
    "线性正向模块（在所有数据中均进行向量化）的计算按照以下公式：\n",
    "\n",
    "$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$\n",
    "\n",
    "其中 $A^{[0]} = X$. \n",
    "\n",
    "**练习**：建立正向传播的线性部分。\n",
    "\n",
    "**提醒**：\n",
    "该单元的数学表示为 $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$，你可能会发现`np.dot（）`有用。 如果维度不匹配，则可以print`W.shape`查看修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6587A4FD3A6F4D12884382D5B3612046",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A) + b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "40B83BC25E4D4D538EFA2CFBA3A0E0E4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2400CA3481514EBFB51BB48271865E23",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**:\n",
    "Z = [[ 3.26295337 -1.23429987]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BA26E425050437A80954B2C63D1B931",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 4.2 正向线性激活\n",
    "\n",
    "在此笔记本中，你将使用两个激活函数：\n",
    "\n",
    "- **Sigmoid**：$\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$。 我们为你提供了“ Sigmoid”函数。 该函数返回**两项值**：激活值\"`a`\"和包含\"`Z`\"的\"`cache`\"（这是我们将馈入到相应的反向函数的内容）。 你可以按下述方式得到两项值：\n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**：ReLu的数学公式为$A = RELU(Z) = max(0, Z)$。我们为你提供了`relu`函数。 该函数返回**两项值**：激活值“`A`”和包含“`Z`”的“`cache`”（这是我们将馈入到相应的反向函数的内容）。 你可以按下述方式得到两项值：\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F20019B0F21E42398DB2E7B4E3736FAB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "为了更加方便，我们把两个函数（线性和激活）组合为一个函数（LINEAR-> ACTIVATION）。 因此，我们将实现一个函数用以执行LINEAR正向步骤和ACTIVATION正向步骤。\n",
    "\n",
    "**练习**：实现 *LINEAR->ACTIVATION* 层的正向传播。 数学表达式为：$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$，其中激活\"g\" 可以是sigmoid（）或relu（）。 使用linear_forward（）和正确的激活函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "32462657CA6D463C88ED40C02BD77C70",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3C44702A3A4048078307FA9338AB238F",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1165CCB17B00412B88EDC2F77219F44E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**:\n",
    "With sigmoid: A = [[0.96890023 0.11013289]]\n",
    "With ReLU: A = [[3.43896131 0.        ]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6B651502E6E4F6CBEF712E1198B131C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    " **注意**：在深度学习中，\"[LINEAR->ACTIVATION]\"计算被视为神经网络中的单个层，而不是两个层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61FCB8C4CB10448BB1779C4EA1AD4CD6",
    "jupyter": {},
    "mdEditEnable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 4.3 L层模型\n",
    "\n",
    "为了方便实现$L$层神经网络，你将需要一个函数来复制前一个函数（使用RELU的`linear_activation_forward`）$L-1$次，以及复制带有SIGMOID的`linear_activation_forward`。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q1d3nmwc2p.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "**图2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* 模型\n",
    "\n",
    "**练习**：实现上述模型的正向传播。\n",
    "\n",
    "**说明**：在下面的代码中，变量`AL`表示$A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$（有时也称为`Yhat`，即$\\hat{Y}$。）\n",
    "\n",
    "**提示**：\n",
    "- 使用你先前编写的函数\n",
    "- 使用for循环复制[LINEAR-> RELU]（L-1）次\n",
    "- 不要忘记在“cache”列表中更新缓存。 要将新值 `c`添加到`list`中，可以使用`list.append(c)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "82BFFA81B51344D1974639FF826171D8",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "         ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev,parameters['W' + str(l)],parameters['b' + str(l)],activation = \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A,parameters['W' + str(L)],parameters['b' + str(L)],activation = \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "637AD566AF9F487CA05CC3F684E92BE2",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.17007265 0.2524272 ]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFBDB6C6F57144C789056F98F731742C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**:\n",
    "AL = [[0.17007265 0.2524272 ]]\n",
    "Length of caches list = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B68BF0F603D4388B63739BC2A088DBB",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "现在，你有了一个完整的正向传播模块，它接受输入X并输出包含预测的行向量$A^{[L]}$。 它还将所有中间值记录在\"caches\"中以计算预测的损失值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B83A8102FDCA410885A6A55F5EA55744",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 5 损失函数\n",
    "\n",
    "现在，你将实现模型的正向和反向传播。 你需要计算损失，以检查模型是否在学习。\n",
    "\n",
    "**练习**：使用以下公式计算交叉熵损失$J$：\n",
    "$$\n",
    "-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EA5A36C3F33B4C3584D9306DB9E38EF1",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = -1 / m * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL),axis=1,keepdims=True)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "31B209FFA877412993265AA78FAD377C",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.41493159961539694\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6C8305AB14C7479ABB154831ED8E623E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**:\n",
    "cost = 0.41493159961539694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17C383959CF64AA188F10A098B98737D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "## 6 反向传播模块\n",
    "\n",
    "就像正向传播一样，你将实现辅助函数以进行反向传播。 请记住，反向传播用于计算损失函数相对于参数的梯度。\n",
    "\n",
    "**提醒**：\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q1d3rwmuaa.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "**图3**：\n",
    "*LINEAR->RELU->LINEAR->SIGMOID* 的正向和反向传播，*紫色块代表正向传播，红色块代表反向传播。*\n",
    "\n",
    "\n",
    "对于那些精通微积分的人（不必进行此作业），可以使用微积分的链式规则来得出2层网络中的损失 $\\mathcal{L}$相对于 $z^{[1]}$的导数，如下所示：\n",
    "\n",
    "$$\n",
    "\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} \n",
    "$$\n",
    "\n",
    "为了计算梯度$dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$，请使用上一个链规则，然后执行$dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$。 在反向传播的每个步骤中，你都将当前梯度乘以对应层的梯度，以获得所需的梯度。\n",
    "\n",
    "同样地，为了计算梯度$db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$，你使用前一个链规则，然后执行$db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$。\n",
    "\n",
    "这也是为什么我们称之为**反向传播**。\n",
    "\n",
    "现在，类似于正向传播，你将分三个步骤构建反向传播：\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward，其中激活函数使用ReLU或sigmoid 的导数计算\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward（整个模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5B685BB998D942FC85B7AB82A8A66C0C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 6.1 线性反向\n",
    "\n",
    "对于层$l$，线性部分为：$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$（之后是激活）。\n",
    "\n",
    "假设你已经计算出导数$dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$。 你想获得$(dW^{[l]}, db^{[l]} dA^{[l-1]})$。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q1d3un62ml.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "**图4 **\n",
    "\n",
    "使用输入$dZ^{[l]}$计算三个输出$(dW^{[l]}, db^{[l]}, dA^{[l]})$。以下是所需的公式：\n",
    "\n",
    "$$ \n",
    "dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}\n",
    "$$\n",
    "$$ \n",
    "db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}\n",
    "$$\n",
    "$$ \n",
    "dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3183322905104D3483031219E9EF0FAE",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**练习**：使用上面的3个公式实现linear_backward（）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FDCB290C48F54A33A6D0A1CF1A6194D4",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "   ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = 1 / m * np.dot(dZ ,A_prev.T)\n",
    "    db = 1 / m * np.sum(dZ,axis = 1 ,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ) \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3D486297BEF1444DB09D715CCA4A60C9",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D20C9AEE3515470FB3CBC5F009568A51",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**: \n",
    "dA_prev = [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]]\n",
    "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
    "db = [[0.50629448]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56DFCDF0664D4817858C3AC549B30617",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 6.2 反向线性激活\n",
    "\n",
    "接下来，创建一个合并两个辅助函数的函数：**`linear_backward`** 和反向步骤的激活 **`linear_activation_backward`**。\n",
    "\n",
    "为了帮助你实现`linear_activation_backward`，我们提供了两个反向函数：\n",
    "- **`sigmoid_backward`**：实现SIGMOID单元的反向传播。 你可以这样使用：\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**：实现RELU单元的反向传播。 你可以这样使用：\n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "如果$g(.)$是激活函数，\n",
    "`sigmoid_backward`和`relu_backward`计算$$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$。\n",
    "\n",
    "**练习**：实现*LINEAR->ACTIVATION* 层的反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "684AB2CE6D7B47A7853BB08FFC6C9E93",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "564CE628D7204E36A24BF5AC98006F4B",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7C3F42A708AE4CF899EAECC62A6A67C1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**sigmoid 输出:**\n",
    "dA_prev = [[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]]\n",
    "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
    "db = [[-0.05729622]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1C23B7226C4D4AA6B7990905FBA981E1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**relu 输出：**\n",
    "dA_prev = [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]]\n",
    "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
    "db = [[-0.20837892]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AB91798C2D1D432183E388BE3147E712",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### 6.3 反向L层模型\n",
    "\n",
    "现在，你将为整个网络实现反向传播函数。 回想一下，当你实现`L_model_forward`函数时，在每次迭代中，你都存储了一个包含（X，W，b和z）的缓存。 在反向传播模块中，你将使用这些变量来计算梯度。 因此，在`L_model_backward`函数中，你将从$L$层开始向后遍历所有隐藏层。在每个步骤中，你都将使用$l$层的缓存值反向传播到层$l$。 图5展示了反向传播过程。\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q1d3wtc6v6.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "**图5**：反向流程\n",
    "\n",
    "**初始化反向传播**：\n",
    "为了使网络反向传播，我们知道输出是\n",
    "$A^{[L]} = \\sigma(Z^{[L]})$。因此，你的代码需要计算`dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$。\n",
    "为此，请使用以下公式（不需要深入的微积分知识）：\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```\n",
    "\n",
    "然后，你可以使用此激活后的梯度`dAL`继续反向传播。如图5所示，你现在可以将`dAL`输入到你实现的LINEAR-> SIGMOID反向函数中（它将使用L_model_forward函数存储的缓存值）。之后，你得通过`for`循环，使用LINEAR-> RELU反向函数迭代所有其他层。同时将每个dA，dW和db存储在grads词典中。为此，请使用以下公式：\n",
    "\n",
    "$$\n",
    "grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} \n",
    "$$\n",
    "\n",
    "例如，当$l=3$时，它将在`grads[\"dW3\"]`中存储 $dW^{[l]}$。 \n",
    "\n",
    "**练习**：实现 *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* 模型的反向传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "94434EC5AFE240E29229A5878D34EB67",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "      \n",
    "    for l in reversed(range(L - 1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+2)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "E2CA308525714702872350D85E6871DF",
    "jupyter": {},
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.          0.52257901]\n",
      " [ 0.         -0.3269206 ]\n",
      " [ 0.         -0.32070404]\n",
      " [ 0.         -0.74079187]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1D3A145A0E546E881530D086BE1D24F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**：\n",
    "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
    " [0.         0.         0.         0.        ]\n",
    " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
    "db1 = [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]]\n",
    "dA1 = [[ 0.          0.52257901]\n",
    " [ 0.         -0.3269206 ]\n",
    " [ 0.         -0.32070404]\n",
    " [ 0.         -0.74079187]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3AFA0075CE94E3882B84084AF360966",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 6.4 更新参数\n",
    "\n",
    "在本节中，你将使用梯度下降来更新模型的参数：\n",
    "\n",
    "$$\n",
    "W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}\n",
    "$$\n",
    "$$ \n",
    "b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}\n",
    "$$\n",
    "\n",
    "其中 $\\alpha$ 是学习率。 在计算更新的参数后，将它们存储在参数字典中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7605AFAEE5FE4AC7812BB634C4B2AAB9",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**练习**：实现`update_parameters（）`以使用梯度下降来更新模型参数。\n",
    "\n",
    "**说明**：\n",
    "对于$l = 1, 2, ..., L$，使用梯度下降更新每个$W^{[l]}$ 和$b^{[l]}$的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "E6705F6606C34A71A0B871346D320ABE",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] =  parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l + 1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l + 1)]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5FB035925AC34596880056AE8A5FB0D9",
    "jupyter": {},
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A434BEF18134B29BB0865DEAC2AF3B1",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出**:\n",
    "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
    "b1 = [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]]\n",
    "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
    "b2 = [[-0.84610769]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3CECD01B54324144BDBFA4BB04828C6E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 7 结论\n",
    "\n",
    "恭喜你实现了构建深度神经网络所需的所有函数！\n",
    "\n",
    "我们知道这是一项艰巨的任务，但是继续前进将变得更好。 下一部分的作业相对容易。\n",
    "\n",
    "在下一个作业中，你将使用这些函数构建两个模型用于分类猫图像和非猫图像：\n",
    "- 两层神经网络\n",
    "- L层神经网络\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
