{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BBBF62F1F3E49939E7307018224A1EC",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 字母级语言模型 - Dinosaurus land\n",
    "\n",
    "欢迎来到恐龙大陆！ 6500万年前，恐龙就已经存在，并且在该作业下它们又回来了。假设你负责一项特殊任务，领先的生物学研究人员正在创造新的恐龙品种，并计划将它们带入地球，而你的工作就是为这些新恐龙起名字。如果恐龙不喜欢它的名字，它可能会变得疯狂，所以需要明智地选择！\n",
    "\n",
    "![Image](../image/L5W1T2/1.jpg)\n",
    "\n",
    "幸运的是，你掌握了深度学习的一些知识，你将使用它来节省时间。你的助手已收集了他们可以找到的所有恐龙名称的列表，并将其编译到此[dataset](dinos.txt)中。（请单击上一个链接查看）要创建新的恐龙名称，你将构建一个字母级语言模型来生成新名称。你的算法将学习不同的名称模式，并随机生成新名称。希望该算法可以使你和你的团队免受恐龙的愤怒！\n",
    "\n",
    "完成此作业，你将学习：\n",
    "\n",
    "- 如何存储文本数据以供RNN使用\n",
    "- 如何在每个时间步采样预测并将其传递给下一个RNN单元以合成数据\n",
    "- 如何建立一个字母级的文本生成循环神经网络\n",
    "- 为什么梯度裁剪很重要\n",
    "\n",
    "我们将从加载`rnn_utils`中为你提供的一些函数开始。具体来说，你可以访问诸如`rnn_forward`和`rnn_backward`之类的函数，这些函数与你在上一个作业中实现的函数等效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E90DC2A1FB4945E08B9A28ADDB772DEF",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\deep learning\\deeplearning.ai\\data\\L5W1T2\n"
     ]
    }
   ],
   "source": [
    "cd ../data/L5W1T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "E6D75B35B7B548F1A93C8715E0A1AB90",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from utils import *\n",
    "import random\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "F3378C7594044B699241594540C6CCA5",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1 问题陈述\n",
    "\n",
    "### 1.1 数据集和预处理\n",
    "\n",
    "运行以下单元格以读取包含恐龙名称的数据集，创建唯一字符列表（例如a-z），并计算数据集和词汇量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0516050D913A4A3E9CC425BD7F800FE7",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19909 total characters and 27 unique characters in your data.\n"
     ]
    }
   ],
   "source": [
    "data = open('dinos.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('There are %d total characters and %d unique characters in your data.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FBC4AD010D845C3B8FBA5D540AFC16F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "这些字符是a-z（26个字符）加上“\\n”（换行符），在此作业中，其作用类似于我们在讲座中讨论的`<EOS>`（句子结尾）标记，仅在此处表示恐龙名称的结尾，而不是句子的结尾。在下面的单元格中，我们创建一个python字典（即哈希表），以将每个字符映射为0-26之间的索引。我们还创建了第二个python字典，该字典将每个索引映射回对应的字符。这将帮助你找出softmax层的概率分布输出中哪个索引对应于哪个字符。下面的`char_to_ix`和`ix_to_char`是python字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9FF0BE1AA75C41F4AE087056B7EA2725",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}\n",
      "{'\\n': 0, 'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }\n",
    "print(ix_to_char)\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01718F7A860242E7B9F929E1AAD535D2",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 1.2 模型概述\n",
    "\n",
    "你的模型将具有以下结构：\n",
    "\n",
    "- 初始化参数\n",
    "- 运行优化循环\n",
    "\t*     正向传播以计算损失函数\n",
    "\t*     反向传播以计算相对于损失函数的梯度\n",
    "\t*     剪裁梯度以避免梯度爆炸\n",
    "\t*     使用梯度下降方法更新参数。\n",
    "- 返回学习的参数\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q1z6qwq7fq.png?imageView2/0/w/960/h/960)\n",
    "\n",
    "**图1**：循环神经网络，类似于你在上一个笔记本“手把手实现循环神经网络”中构建的内容。\n",
    "\n",
    "在每个时间步，RNN都会根据给定的先前字符来预测下一个字符。数据集$X = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$是训练集中的字符列表，而$Y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$使得每个时间步$t$，我们有$y^{\\langle t \\rangle} = x^{\\langle t+1 \\rangle}$。\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95E65EAF900E434B8C8768F590DD154D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 2 构建模型模块\n",
    "\n",
    "在这一部分中，你将构建整个模型的两个重要模块：\n",
    "- 梯度裁剪：避免梯度爆炸\n",
    "- 采样：一种用于生成字符的技术\n",
    "\n",
    "然后，你将应用这两个函数来构建模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC056068C0154F8A98982F7A3A03DD80",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.1 在优化循环中裁剪梯度\n",
    "\n",
    "在本节中，你将实现在优化循环中调用的`clip`函数。回想一下，你的总体循环结构通常由正向传播，损失计算，反向传播和参数更新组成。在更新参数之前，你将在需要时执行梯度裁剪，以确保你的梯度不会“爆炸”，这意味着要采用很大的值。\n",
    "\n",
    "在下面的练习中，你将实现一个函数`clip`，该函数接收梯度字典，并在需要时返回裁剪后的梯度。梯度裁剪有多种方法。我们将使用简单的按元素裁剪程序，其中将梯度向量的每个元素裁剪为位于范围[-N，N]之间。通常，你将提供一个`maxValue`（例如10）。在此示例中，如果梯度向量的任何分量大于10，则将其设置为10；并且如果梯度向量的任何分量小于-10，则将其设置为-10。如果介于-10和10之间，则将其保留。\n",
    "\n",
    "![Image](../image/L5W1T2/1.png)\n",
    "\n",
    "**图2**：在网络遇到轻微的“梯度爆炸”的情况下，使用与不使用梯度裁剪的梯度下降对比。\n",
    "\n",
    "**练习**：实现以下函数以返回字典`gradients`的裁剪梯度。你的函数接受最大阈值，并返回裁剪后的梯度。你可以查看此[hint](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.clip.html)，以获取有关如何裁剪numpy的示例。你将需要使用参数`out = ...`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AE57C340BB8040B88978A60DEEE8362F",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### GRADED FUNCTION: clip\n",
    "\n",
    "def clip(gradients, maxValue):\n",
    "    '''\n",
    "    Clips the gradients' values between minimum and maximum.\n",
    "    \n",
    "    Arguments:\n",
    "    gradients -- a dictionary containing the gradients \"dWaa\", \"dWax\", \"dWya\", \"db\", \"dby\"\n",
    "    maxValue -- everything above this number is set to this number, and everything less than -maxValue is set to -maxValue\n",
    "    \n",
    "    Returns: \n",
    "    gradients -- a dictionary with the clipped gradients.\n",
    "    '''\n",
    "    \n",
    "    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']\n",
    "   \n",
    "    ### START CODE HERE ###\n",
    "    # clip to mitigate exploding gradients, loop over [dWax, dWaa, dWya, db, dby]. (≈2 lines)\n",
    "    for gradient in [dWax, dWaa, dWya, db, dby]:\n",
    "        np.clip(gradient,-maxValue , maxValue, out=gradient)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "63E565ABC98F43D7994196A4F33AFB2B",
    "jupyter": {},
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dWaa\"][1][2] = 10.0\n",
      "gradients[\"dWax\"][3][1] = -10.0\n",
      "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
      "gradients[\"db\"][4] = [10.]\n",
      "gradients[\"dby\"][1] = [8.45833407]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3)\n",
    "dWax = np.random.randn(5,3)*10\n",
    "dWaa = np.random.randn(5,5)*10\n",
    "dWya = np.random.randn(2,5)*10\n",
    "db = np.random.randn(5,1)*10\n",
    "dby = np.random.randn(2,1)*10\n",
    "gradients = {\"dWax\": dWax, \"dWaa\": dWaa, \"dWya\": dWya, \"db\": db, \"dby\": dby}\n",
    "gradients = clip(gradients, 10)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients[\"dWax\"][3][1])\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1C12F3D4B09B4D44946B4EBED1B5328C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出:**\n",
    "gradients[\"dWaa\"][1][2] = 10.0\n",
    "gradients[\"dWax\"][3][1] = -10.0\n",
    "gradients[\"dWya\"][1][2] = 0.2971381536101662\n",
    "gradients[\"db\"][4] = [10.]\n",
    "gradients[\"dby\"][1] = [8.45833407]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AC1F1B1C9A14BABAFFFF1BE77EF78D6",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 2.2 采样\n",
    "\n",
    "现在假设你的模型已经训练好。你想生成新文本（字符）。下图说明了生成过程：\n",
    "![Image](../image/L5W1T2/2.png)\n",
    "**图3**：在此图中，我们假设模型已经训练好。我们在第一步中传入$x^{\\langle 1\\rangle} = \\vec{0}$，然后让网络一次采样一个字符。\n",
    "\n",
    "**练习**：实现以下的`sample`函数来采样字母。你需要执行4个步骤：\n",
    "\n",
    "- **步骤1**：将第一个\"dummy\"输入$x^{\\langle 1 \\rangle} = \\vec{0}$（零向量）传递给网络。这是我们生成任意字母之前的默认输入。我们还设置$a^{\\langle 0 \\rangle} = \\vec{0}$。\n",
    "\n",
    "- **步骤2**：执行向正向传播的步骤，即可获得$a^{\\langle 1 \\rangle}$ and $\\hat{y}^{\\langle 1 \\rangle}$。以下是等式：\n",
    "\n",
    "$$\n",
    "a^{\\langle t+1 \\rangle} = \\tanh(W_{ax}  x^{\\langle t \\rangle } + W_{aa} a^{\\langle t \\rangle } + b)\\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "z^{\\langle t + 1 \\rangle } = W_{ya}  a^{\\langle t + 1 \\rangle } + b_y \\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{y}^{\\langle t+1 \\rangle } = softmax(z^{\\langle t + 1 \\rangle })\\tag{3}\n",
    "$$\n",
    "\n",
    "注意$\\hat{y}^{\\langle t+1 \\rangle }$是一个（softmax）概率向量（其条目在0到1之间且总和为1）。$\\hat{y}^{\\langle t+1 \\rangle}_i$ 表示由\"i\"索引的字符是下一个字符的概率。我们提供了一个`softmax（）`函数供你使用。\n",
    "\n",
    "- **步骤3**：执行采样：根据$\\hat{y}^{\\langle t+1 \\rangle }$指定的概率分布，选择下一个字符的索引。这意味着，如果$\\hat{y}^{\\langle t+1 \\rangle }_i = 0.16$，你将以16％的概率选择索引\"i\"。要实现它，你可以使用[`np.random.choice`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.choice.html)。\n",
    "\n",
    "以下是一个使用`np.random.choice()`的例子：\n",
    "```python\n",
    "np.random.seed(0)\n",
    "p = np.array([0.1, 0.0, 0.7, 0.2])\n",
    "index = np.random.choice([0, 1, 2, 3], p = p.ravel())\n",
    "```\n",
    "这意味着你将根据分布选择`index`：\n",
    "$P(index = 0) = 0.1, P(index = 1) = 0.0, P(index = 2) = 0.7, P(index = 3) = 0.2$。\n",
    "\n",
    "- **步骤4**：要在`sample()`中实现的最后一步是覆盖变量`x`，该变量当前存储$x^{\\langle t \\rangle }$，其值为$x^{\\langle t + 1 \\rangle }$。通过创建与预测字符相对应的独热向量以表示$x^{\\langle t + 1 \\rangle }$。然后，你将在步骤1中前向传播$x^{\\langle t + 1 \\rangle }$，并继续重复此过程，直到获得“\\n”字符，表明你已经到达恐龙名称的末尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "BB3B7AA5B02A44E7894C205F5FBEE4A0",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sample\n",
    "\n",
    "def sample(parameters, char_to_ix, seed):\n",
    "    \"\"\"\n",
    "    Sample a sequence of characters according to a sequence of probability distributions output of the RNN\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing the parameters Waa, Wax, Wya, by, and b. \n",
    "    char_to_ix -- python dictionary mapping each character to an index.\n",
    "    seed -- used for grading purposes. Do not worry about it.\n",
    "\n",
    "    Returns:\n",
    "    indices -- a list of length n containing the indices of the sampled characters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve parameters and relevant shapes from \"parameters\" dictionary\n",
    "    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Step 1: Create the one-hot vector x for the first character (initializing the sequence generation). (≈1 line)\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # Step 1': Initialize a_prev as zeros (≈1 line)\n",
    "    a_prev = np.zeros((n_a,1))\n",
    "    \n",
    "    # Create an empty list of indices, this is the list which will contain the list of indices of the characters to generate (≈1 line)\n",
    "    indices = []\n",
    "    \n",
    "    # Idx is a flag to detect a newline character, we initialize it to -1\n",
    "    idx = -1 \n",
    "    \n",
    "    # Loop over time-steps t. At each time-step, sample a character from a probability distribution and append \n",
    "    # its index to \"indices\". We'll stop if we reach 50 characters (which should be very unlikely with a well \n",
    "    # trained model), which helps debugging and prevents entering an infinite loop. \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        # Step 2: Forward propagate x using the equations (1), (2) and (3)\n",
    "        a = np.tanh(np.dot(Wax,x)+np.dot(Waa,a_prev)+b)\n",
    "        z = np.dot(Wya,a)+by\n",
    "        y = softmax(z)\n",
    "        \n",
    "        # for grading purposes\n",
    "        np.random.seed(counter+seed) \n",
    "        \n",
    "        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y\n",
    "        idx = np.random.choice(range(len(y)),p=y.ravel())\n",
    "\n",
    "\n",
    "        # Append the index to \"indices\"\n",
    "        indices.append(idx)\n",
    "        \n",
    "        # Step 4: Overwrite the input character as the one corresponding to the sampled index.\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[idx] = 1\n",
    "        \n",
    "        # Update \"a_prev\" to be \"a\"\n",
    "        a_prev = a\n",
    "        \n",
    "        # for grading purposes\n",
    "        seed += 1\n",
    "        counter +=1\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "E208DEFC676143368B691D2098268AB1",
    "jupyter": {},
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling:\n",
      "list of sampled indices: [18, 2, 26, 0]\n",
      "list of sampled characters: ['r', 'b', 'z', '\\n']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "n, n_a = 20, 100\n",
    "a0 = np.random.randn(n_a, 1)\n",
    "i0 = 1 # first character is ix_to_char[i0]\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "\n",
    "\n",
    "indices = sample(parameters, char_to_ix, 0)\n",
    "print(\"Sampling:\")\n",
    "print(\"list of sampled indices:\", indices)\n",
    "print(\"list of sampled characters:\", [ix_to_char[i] for i in indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91DC31494DA2475188B7B426F944A205",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出:**\n",
    "Sampling:\n",
    "list of sampled indices: [18, 2, 26, 0]\n",
    "list of sampled characters: ['r', 'b', 'z', '\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C242E3A05F0942578F9141B58E14273F",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 3 建立语言模型\n",
    "\n",
    "现在是时候建立用于文字生成的字母级语言模型了。\n",
    "\n",
    "### 3.1 梯度下降\n",
    "\n",
    "在本部分中，你将实现一个函数，该函数执行随机梯度下降的一个步骤（梯度裁剪）。你将一次查看一个训练示例，因此优化算法为随机梯度下降。提醒一下，以下是RNN常见的优化循环的步骤：\n",
    "\n",
    "- 通过RNN正向传播以计算损失\n",
    "- 随时间反向传播以计算相对于参数的损失梯度\n",
    "- 必要时裁剪梯度\n",
    "- 使用梯度下降更新参数\n",
    "\n",
    "**练习**：实现此优化过程（随机梯度下降的一个步骤）。\n",
    "\n",
    "我们为你提供了以下函数：\n",
    "\n",
    "```python\n",
    "def rnn_forward(X, Y, a_prev, parameters):\n",
    "    \"\"\" Performs the forward propagation through the RNN and computes the cross-entropy loss.\n",
    "    It returns the loss' value as well as a \"cache\" storing values to be used in the backpropagation.\"\"\"\n",
    "    ....\n",
    "    return loss, cache\n",
    "    \n",
    "def rnn_backward(X, Y, parameters, cache):\n",
    "    \"\"\" Performs the backward propagation through time to compute the gradients of the loss with respect\n",
    "    to the parameters. It returns also all the hidden states.\"\"\"\n",
    "    ...\n",
    "    return gradients, a\n",
    "\n",
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\" Updates parameters using the Gradient Descent Update Rule.\"\"\"\n",
    "    ...\n",
    "    return parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "276309E8623A48C98C351E67AF5E3346",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):\n",
    "    \"\"\"\n",
    "    Execute one step of the optimization to train the model.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- list of integers, where each integer is a number that maps to a character in the vocabulary.\n",
    "    Y -- list of integers, exactly the same as X but shifted one index to the left.\n",
    "    a_prev -- previous hidden state.\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    learning_rate -- learning rate for the model.\n",
    "    \n",
    "    Returns:\n",
    "    loss -- value of the loss function (cross-entropy)\n",
    "    gradients -- python dictionary containing:\n",
    "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
    "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
    "                        dWya -- Gradients of hidden-to-output weights, of shape (n_y, n_a)\n",
    "                        db -- Gradients of bias vector, of shape (n_a, 1)\n",
    "                        dby -- Gradients of output bias vector, of shape (n_y, 1)\n",
    "    a[len(X)-1] -- the last hidden state, of shape (n_a, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Forward propagate through time (≈1 line)\n",
    "    loss, cache = rnn_forward(X,Y,a_prev,parameters)\n",
    "    \n",
    "    # Backpropagate through time (≈1 line)\n",
    "    gradients, a = rnn_backward(X,Y,parameters,cache)\n",
    "    \n",
    "    # Clip your gradients between -5 (min) and 5 (max) (≈1 line)\n",
    "    gradients = clip(gradients,5)\n",
    "    \n",
    "    # Update parameters (≈1 line)\n",
    "    parameters = update_parameters(parameters,gradients,learning_rate)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss, gradients, a[len(X)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "C3CBBC69541E4770A432971E988626AC",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 126.50397572165345\n",
      "gradients[\"dWaa\"][1][2] = 0.19470931534725341\n",
      "np.argmax(gradients[\"dWax\"]) = 93\n",
      "gradients[\"dWya\"][1][2] = -0.007773876032004315\n",
      "gradients[\"db\"][4] = [-0.06809825]\n",
      "gradients[\"dby\"][1] = [0.01538192]\n",
      "a_last[4] = [-1.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "vocab_size, n_a = 27, 100\n",
    "a_prev = np.random.randn(n_a, 1)\n",
    "Wax, Waa, Wya = np.random.randn(n_a, vocab_size), np.random.randn(n_a, n_a), np.random.randn(vocab_size, n_a)\n",
    "b, by = np.random.randn(n_a, 1), np.random.randn(vocab_size, 1)\n",
    "parameters = {\"Wax\": Wax, \"Waa\": Waa, \"Wya\": Wya, \"b\": b, \"by\": by}\n",
    "X = [12,3,5,11,22,3]\n",
    "Y = [4,14,11,22,25, 26]\n",
    "\n",
    "loss, gradients, a_last = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)\n",
    "print(\"Loss =\", loss)\n",
    "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients[\"dWaa\"][1][2])\n",
    "print(\"np.argmax(gradients[\\\"dWax\\\"]) =\", np.argmax(gradients[\"dWax\"]))\n",
    "print(\"gradients[\\\"dWya\\\"][1][2] =\", gradients[\"dWya\"][1][2])\n",
    "print(\"gradients[\\\"db\\\"][4] =\", gradients[\"db\"][4])\n",
    "print(\"gradients[\\\"dby\\\"][1] =\", gradients[\"dby\"][1])\n",
    "print(\"a_last[4] =\", a_last[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "152447EF842D4177A710D8C0FDE67395",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**预期输出:**\n",
    "Loss = 126.50397572165363\n",
    "gradients[\"dWaa\"][1][2] = 0.19470931534719205\n",
    "np.argmax(gradients[\"dWax\"]) = 93\n",
    "gradients[\"dWya\"][1][2] = -0.007773876032003275\n",
    "gradients[\"db\"][4] = [-0.06809825]\n",
    "gradients[\"dby\"][1] = [0.01538192]\n",
    "a_last[4] = [-1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "1A23E25257704C0293B409768C838028",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### 3.2 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BB9392AAAC5C431286B1FF5A34266A8C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "给定恐龙名称数据集，我们将数据集的每一行（一个名称）用作一个训练示例。每100步随机梯度下降，你将抽样10个随机选择的名称，以查看算法的运行情况。请记住要对数据集进行混洗，以便随机梯度下降以随机顺序访问示例。\n",
    "\n",
    "**练习**：按照说明进行操作并实现`model()`。当`examples [index]`包含一个恐龙名称（字符串）时，创建示例（X，Y），可以使用以下方法：\n",
    "```python\n",
    "        index = j % len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]] \n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "```\n",
    "注意，我们使用：`index= j % len(examples)`，其中`j = 1....num_iterations`，以确保`examples [index]`始终是有效的语句（`index`小于`len(examples)`）。\n",
    "`X`的第一个条目为None将被`rnn_forward()`解释为设置$x^{\\langle 0 \\rangle} = \\vec{0}$。此外，这确保了`Y`等于`X`，但向左移动了一步，并附加了“\\n”以表示恐龙名称的结尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "F7FA32E0BA294A038707B136CBA5BEA2",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):\n",
    "    \"\"\"\n",
    "    Trains the model and generates dinosaur names. \n",
    "    \n",
    "    Arguments:\n",
    "    data -- text corpus\n",
    "    ix_to_char -- dictionary that maps the index to a character\n",
    "    char_to_ix -- dictionary that maps a character to an index\n",
    "    num_iterations -- number of iterations to train the model for\n",
    "    n_a -- number of units of the RNN cell\n",
    "    dino_names -- number of dinosaur names you want to sample at each iteration. \n",
    "    vocab_size -- number of unique characters found in the text, size of the vocabulary\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- learned parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve n_x and n_y from vocab_size\n",
    "    n_x, n_y = vocab_size, vocab_size\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)\n",
    "    loss = get_initial_loss(vocab_size, dino_names)\n",
    "    \n",
    "    # Build list of all dinosaur names (training examples).\n",
    "    with open(\"dinos.txt\") as f:\n",
    "        examples = f.readlines()\n",
    "    examples = [x.lower().strip() for x in examples]\n",
    "    \n",
    "    # Shuffle list of all dinosaur names\n",
    "    shuffle(examples)\n",
    "    \n",
    "    # Initialize the hidden state of your LSTM\n",
    "    a_prev = np.zeros((n_a, 1))\n",
    "    \n",
    "    # Optimization loop\n",
    "    for j in range(num_iterations):\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        \n",
    "        # Use the hint above to define one training example (X,Y) (≈ 2 lines)\n",
    "        index = j%len(examples)\n",
    "        X = [None] + [char_to_ix[ch] for ch in examples[index]]\n",
    "        Y = X[1:] + [char_to_ix[\"\\n\"]]\n",
    "        \n",
    "        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters\n",
    "        # Choose a learning rate of 0.01\n",
    "        curr_loss, gradients, a_prev = optimize(X,Y,a_prev,parameters,learning_rate=0.01)  \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.\n",
    "        loss = smooth(loss, curr_loss)\n",
    "\n",
    "        # Every 2000 Iteration, generate \"n\" characters thanks to sample() to check if the model is learning properly\n",
    "        if j % 2000 == 0:\n",
    "            \n",
    "            print('Iteration: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            \n",
    "            # The number of dinosaur names to print\n",
    "            seed = 0\n",
    "            for name in range(dino_names):\n",
    "                \n",
    "                # Sample indices and print them\n",
    "                sampled_indices = sample(parameters, char_to_ix, seed)\n",
    "                print_sample(sampled_indices, ix_to_char)\n",
    "                \n",
    "                seed += 1  # To get the same result for grading purposed, increment the seed by one. \n",
    "      \n",
    "            print('\\n')\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FF73369A7074CF288C3C95829B6BE2C",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "运行以下单元格，你应该观察到模型在第一次迭代时输出看似随机的字符。经过数千次迭代后，你的模型应该学会生成看起来合理的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "24653FEAC8A44E138B4CEEE32B5B2ADA",
    "jupyter": {},
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Loss: 23.093928\n",
      "\n",
      "Nkzxwtdmfqoeyhsqwasjkjvu\n",
      "Kneb\n",
      "Kzxwtdmfqoeyhsqwasjkjvu\n",
      "Neb\n",
      "Zxwtdmfqoeyhsqwasjkjvu\n",
      "Eb\n",
      "Xwtdmfqoeyhsqwasjkjvu\n",
      "\n",
      "\n",
      "Iteration: 2000, Loss: 28.021806\n",
      "\n",
      "Nhytrongdos\n",
      "Ipca\n",
      "Jytrongdos\n",
      "Nacalrus\n",
      "Xusairgoravgtos\n",
      "Ca\n",
      "Tromieronxesauius\n",
      "\n",
      "\n",
      "Iteration: 4000, Loss: 25.975095\n",
      "\n",
      "Nivtosaurus\n",
      "Jnecalosaurus\n",
      "Kxusoeonosaurus\n",
      "Necalosaurus\n",
      "Xusodonosaurus\n",
      "Caagosauoshusamlus\n",
      "Sspangosaurus\n",
      "\n",
      "\n",
      "Iteration: 6000, Loss: 24.678191\n",
      "\n",
      "Phytrodon\n",
      "Lmda\n",
      "Mytroeratong\n",
      "Pecalropderus\n",
      "Xustaresceurusnatincos\n",
      "Ecaison\n",
      "Trodon\n",
      "\n",
      "\n",
      "Iteration: 8000, Loss: 24.282649\n",
      "\n",
      "Onyusdchasgitarosaurus\n",
      "Lnacalosaurus\n",
      "Lyushancinaurus\n",
      "Olaahosaurus\n",
      "Xussaparonx\n",
      "Edalosaurus\n",
      "Trodolosaurus\n",
      "\n",
      "\n",
      "Iteration: 10000, Loss: 23.995227\n",
      "\n",
      "Ngyusia\n",
      "Liba\n",
      "Lyusiachinexasiucopherpa\n",
      "Omaaisaurus\n",
      "Yusodon\n",
      "Ecamosaurus\n",
      "Strcheosaurus\n",
      "\n",
      "\n",
      "Iteration: 12000, Loss: 23.591464\n",
      "\n",
      "Mexutodon\n",
      "Koebahosaurus\n",
      "Lytosaurus\n",
      "Macagoton\n",
      "Yustangosaurus\n",
      "Daafosaurus\n",
      "Trocheptos\n",
      "\n",
      "\n",
      "Iteration: 14000, Loss: 23.544010\n",
      "\n",
      "Onyxroinatops\n",
      "Licebatophaptor\n",
      "Lustoinericulosharhinrochusimechejueliadia\n",
      "Oja\n",
      "Xuspendosaurus\n",
      "Ecaesona\n",
      "Streonosaurus\n",
      "\n",
      "\n",
      "Iteration: 16000, Loss: 23.323154\n",
      "\n",
      "Mivrysaurus\n",
      "Llacaiosaurus\n",
      "Lustodon\n",
      "Mecaeosaurus\n",
      "Xuspamarncripmuangiatocesaurus\n",
      "Edacosaurus\n",
      "Stramchodycips\n",
      "\n",
      "\n",
      "Iteration: 18000, Loss: 23.102992\n",
      "\n",
      "Mavuskanatonus\n",
      "Liabahosaurus\n",
      "Lurosaurus\n",
      "Mabaasia\n",
      "Wustaraptos\n",
      "Eaabisaurus\n",
      "Stranbosaurus\n",
      "\n",
      "\n",
      "Iteration: 20000, Loss: 23.116090\n",
      "\n",
      "Lixusochesheunorocilanspalonxilandoun\n",
      "Liecalosaurus\n",
      "Lustolleronsaurus\n",
      "Mecafpria\n",
      "Wurodonophus\n",
      "Edajosaurus\n",
      "Strengopethops\n",
      "\n",
      "\n",
      "Iteration: 22000, Loss: 22.913513\n",
      "\n",
      "Meyrosaurus\n",
      "Lieeaisaurhisaurus\n",
      "Lyusicheosaurus\n",
      "Madadroma\n",
      "Yurocenitaurus\n",
      "Edacosaurus\n",
      "Trocheosaurus\n",
      "\n",
      "\n",
      "Iteration: 24000, Loss: 22.860622\n",
      "\n",
      "Mixus\n",
      "Libabason\n",
      "Lustrionosaurus\n",
      "Medacosaurus\n",
      "Wuroceratos\n",
      "Edadronaalunoneothaculastagys\n",
      "Straolicetatos\n",
      "\n",
      "\n",
      "Iteration: 26000, Loss: 22.670344\n",
      "\n",
      "Meytrodon\n",
      "Liceachula\n",
      "Lyxus\n",
      "Macahosaurus\n",
      "Wurodon\n",
      "Edadroma\n",
      "Strandon\n",
      "\n",
      "\n",
      "Iteration: 28000, Loss: 22.746078\n",
      "\n",
      "Nixushaperiatgus\n",
      "Liceacts\n",
      "Lyxusaurus\n",
      "Necalrus\n",
      "Wusterasheurusiatengosaurus\n",
      "Ecaisue\n",
      "Trochernaujusapranhus\n",
      "\n",
      "\n",
      "Iteration: 30000, Loss: 22.751358\n",
      "\n",
      "Ngxstongheogukusmaranosaurus\n",
      "Liecamprodantosaurus\n",
      "Lustonganmauhortathonmoceravopdleisaurus\n",
      "Ncacerradcosaurus\n",
      "Wurogolosaurus\n",
      "Edaeropadroptorix\n",
      "Tordichodratosaurus\n",
      "\n",
      "\n",
      "Iteration: 32000, Loss: 22.662760\n",
      "\n",
      "Niustresaurus\n",
      "Loeg\n",
      "Lvstrasaurus\n",
      "Necansteg\n",
      "Wurodonsaurus\n",
      "Ehalosaurus\n",
      "Striratops\n",
      "\n",
      "\n",
      "Iteration: 34000, Loss: 22.539116\n",
      "\n",
      "Nivosaurus\n",
      "Licecasphiansaurus\n",
      "Lurosaurus\n",
      "Necalosaurus\n",
      "Wrrodon\n",
      "Eeaclsan\n",
      "Strenjosaurus\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = model(data, ix_to_char, char_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "662B6E09341C495F8FB8A188E326EE42",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 结论\n",
    "你可以看到，在训练即将结束时，你的算法已开始生成合理的恐龙名称。刚开始时，它会生成随机字符，但是到最后，你会看到恐龙名字的结尾很酷。运行该算法更长时间，并调整超参数来看看是否可以获得更好的结果。我们的实现产生了一些非常酷的名称，例如“maconucon”，“marloralus”和“macingsersaurus”。你的模型还有望了解到恐龙名称往往以`saurus`，`don`，`aura`，`tor`等结尾。\n",
    "\n",
    "如果你的模型生成了一些不酷的名字，请不要完全怪罪模型-并非所有实际的恐龙名字听起来都很酷。（例如，dromaeosauroides是实际存在的恐龙名称，并且也在训练集中。）但是此模型应该给你提供了一组可以从中挑选的候选名字！\n",
    "\n",
    "该作业使用了相对较小的数据集，因此你可以在CPU上快速训练RNN。训练英语模型需要更大的数据集，并且通常需要更多的计算，在GPU上也要运行多个小时。我们使用恐龙的名字已经有一段时间了，到目前为止，我们最喜欢的名字是great, undefeatable,且fierce的：Mangosaurus!\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q1z6w2f3dj.jpeg?imageView2/0/w/960/h/960)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C9EFBEE0E74F4740997E6760BCC8D32B",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 4 像莎士比亚一样创作\n",
    "\n",
    "该笔记本的其余部分是可选的，尚未评分，但我们希望你都尝试做一下，因为它非常有趣且内容丰富。\n",
    "\n",
    "一个类似（但更复杂）的任务是生成莎士比亚诗歌。无需从恐龙名称的数据集中学习，而是使用莎士比亚诗歌集。使用LSTM单元，你可以学习跨文本中许多字符的长期依赖关系。例如，某个字符出现在序列的某个地方可能会影响序列后面的其他字符。这些长期依赖关系对于恐龙名称来说不太重要，因为它们的名称很短。\n",
    "![Image Name](https://cdn.kesci.com/upload/image/q1z6wks037.jpg?imageView2/0/w/960/h/960)\n",
    "\n",
    "让我们成为诗人！\n",
    "\n",
    "我们已经用Keras实现了莎士比亚诗歌生成器。运行以下单元格以加载所需的软件包和模型。这可能需要几分钟的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "95013944E80A431D8AF23EC779AAB2BA",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data...\n",
      "Creating training set...\n",
      "number of training examples: 31412\n",
      "Vectorizing training set...\n",
      "Loading model...\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\machineLearing\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\machineLearing\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\machineLearing\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\machineLearing\\lib\\site-packages\\keras\\engine\\saving.py:384: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model, load_model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Input, Masking\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from shakespeare_utils import *\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3CC9872D9A242D9A00B2EC0276934D4",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "为了节省你的时间，我们已经在莎士比亚的十四行诗[*\"The Sonnets\"*](shakespeare.txt)诗歌集上训练了大约1000个epoch的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "430009895A99482E84B584DB7B56681E",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "让我们再训练模型完成一个新epoch，这也将花费几分钟。你可以运行`generate_output`，这将提示你输入小于40个字符的句子。这首诗将从你输入的句子开始，我们的RNN-Shakespeare将为你完成其余的部分！例如，尝试\"Forsooth this maketh no sense \"（不要输入引号）。根据是否在末尾加上空格，你的结果也可能会有所不同，两种方法都应尝试，也可以尝试其他输入法。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "C4683AF3BE6D498398F1437D47DFAE12",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "31412/31412 [==============================] - 43s 1ms/step - loss: 2.7337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1dd08aef608>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "\n",
    "model.fit(x, y, batch_size=128, epochs=1, callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "9454461B49A74867860B98F088FEC0BB",
    "jupyter": {},
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your poem, the Shakespeare machine will complete it. Your input is: what?\n",
      "\n",
      "\n",
      "Here is your poem: \n",
      "\n",
      "what?\n",
      "heanterped the beil moaliis forned anse bork,\n",
      "affoul of a dother mase nof fuss not in thy awont,\n",
      "and whele you conse woml'st them leantich thee,\n",
      "whicg in that hil tonce wes of with's made love?\n",
      " \n",
      "shemviess magit goud wills in my hist i forsh the worl!\n",
      "lo wo be be a swors to what i fike thy liye,\n",
      "and be afthought all she know saccare,\n",
      "that withrer by gail's kills have of bidy,\n",
      "sain to yet whe time"
     ]
    }
   ],
   "source": [
    "# Run this cell to try with different inputs without having to re-train the model \n",
    "generate_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCDFBD74714645598C805B0F7D315CD8",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "RNN-Shakespeare模型与你为恐龙名称构建的模型非常相似。唯一的区别是：\n",
    "- LSTM代替基本的RNN来捕获更远的依赖\n",
    "- 模型是更深的堆叠的LSTM模型（2层）\n",
    "- 使用Keras而不是python来简化代码\n",
    "\n",
    "如果你想了解更多信息，还可以在GitHub上查看Keras Team的文本生成实现：https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py \n",
    "\n",
    "祝贺你完成本笔记本作业！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "912E807E9EE3492A8DFF4CAADE93965D",
    "jupyter": {},
    "mdEditEnable": false,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "**参考**:\n",
    "- This exercise took inspiration from Andrej Karpathy's implementation: https://gist.github.com/karpathy/d4dee566867f8291f086. To learn more about text generation, also check out Karpathy's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).\n",
    "- For the Shakespearian poem generator, our implementation was based on the implementation of an LSTM text generator by the Keras team: https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machineLearing]",
   "language": "python",
   "name": "conda-env-machineLearing-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
